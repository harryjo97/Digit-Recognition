{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELnwcr_th1zW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import *\n",
    "import os\n",
    "\n",
    "from random import *\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awVEMcgSh1zd"
   },
   "outputs": [],
   "source": [
    "# define train set\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "train = pd.read_csv('./drive/My Drive/DACON/data_file/train.csv')\n",
    "test = pd.read_csv('./drive/My Drive/DACON/data_file/test.csv')\n",
    "train_copy = train.copy()\n",
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2e7FkuFoE7M"
   },
   "outputs": [],
   "source": [
    "rot_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=45, # rotation range 1이 최대로 움직인 각도 : 45도\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0,     # 블로그 펌 ㅎ\n",
    "    zoom_range=0,      # 마찬가지 블로그 펌 ㅎ\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant', # 밀린 부분은 0으로 고정\n",
    "    cval=0.0,             # 밀린 부분에 해당하는 constant\n",
    "    horizontal_flip=False, # 뒤집기\n",
    "    vertical_flip=False,   # 뒤집기2\n",
    "    rescale=1./255, # Rescale\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0, # Valid split ; 나중에 따로 할 필요없음\n",
    "    dtype=None\n",
    ")\n",
    "\n",
    "trans_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0, # rotation range 1이 최대로 움직인 각도 : 45도\n",
    "    width_shift_range=0.2, # 블로그 펌\n",
    "    height_shift_range=0.2,# 블로그 펌\n",
    "    brightness_range=None,\n",
    "    shear_range=0,     # 블로그 펌\n",
    "    zoom_range=0,      # 마찬가지 블로그 펌\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant', # 밀린 부분은 0으로 고정\n",
    "    cval=0.0,             # 밀린 부분에 해당하는 constant\n",
    "    horizontal_flip=False, # 뒤집기\n",
    "    vertical_flip=False,   # 뒤집기2\n",
    "    rescale=1./255, # Rescale\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0, # Valid split ; 나중에 따로 할 필요없음\n",
    "    dtype=None\n",
    ")\n",
    "\n",
    "shear_zoom_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0, # rotation range 1이 최대로 움직인 각도 : 45도\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.2,     # 블로그 펌\n",
    "    zoom_range=0.2,      # 마찬가지 블로그 펌\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant', # 밀린 부분은 0으로 고정\n",
    "    cval=0.0,             # 밀린 부분에 해당하는 constant\n",
    "    horizontal_flip=False, # 뒤집기\n",
    "    vertical_flip=False,   # 뒤집기2\n",
    "    rescale=1./255, # Rescale\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0, # Valid split ; 나중에 따로 할 필요없음\n",
    "    dtype=None\n",
    ")\n",
    "\n",
    "flip_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0, # rotation range 1이 최대로 움직인 각도 : 45도\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0,     # 블로그 펌\n",
    "    zoom_range=0,      # 마찬가지 블로그 펌\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant', # 밀린 부분은 0으로 고정\n",
    "    cval=0.0,             # 밀린 부분에 해당하는 constant\n",
    "    horizontal_flip=True, # 뒤집기\n",
    "    vertical_flip=True,   # 뒤집기2\n",
    "    rescale=1./255, # Rescale\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0, # Valid split ; 나중에 따로 할 필요없음\n",
    "    dtype=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "me8v8S6Aq1R5"
   },
   "outputs": [],
   "source": [
    "def augmentation( input_imgs ):\n",
    "    df = input_imgs\n",
    "    new_data_set = []\n",
    "    num_of_training_set = df.shape[0]\n",
    "\n",
    "    for i in range(num_of_training_set//2):\n",
    "        rand_1 = np.random.randint(num_of_training_set)\n",
    "        rand_2 = np.random.randint(num_of_training_set)\n",
    "        rand_3 = np.random.randint(num_of_training_set)\n",
    "        rand_4 = np.random.randint(num_of_training_set)\n",
    "    \n",
    "        for j in range(3):\n",
    "            # rotation\n",
    "            _rot = rot_gen.flow( np.array(df.iloc[rand_1,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n",
    "            new_data_set += [[\n",
    "                df.iloc[rand_1,1],\n",
    "                df.iloc[rand_1,2],\n",
    "            ] + list(_rot)]\n",
    "            # translation\n",
    "            _trans = trans_gen.flow( np.array(df.iloc[rand_2,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n",
    "            new_data_set += [[\n",
    "                df.iloc[rand_2,1],\n",
    "                df.iloc[rand_2,2],\n",
    "            ] + list(_trans)]\n",
    "            # shear / zoom\n",
    "            _shear = shear_zoom_gen.flow( np.array(df.iloc[rand_3,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n",
    "            new_data_set += [[\n",
    "                df.iloc[rand_3,1],\n",
    "                df.iloc[rand_3,2],\n",
    "            ] + list(_shear)]\n",
    "            # flip\n",
    "            _flip = flip_gen.flow( np.array(df.iloc[rand_4,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n",
    "            new_data_set += [[\n",
    "                df.iloc[rand_4,1],\n",
    "                df.iloc[rand_4,2],\n",
    "            ] + list(_flip)]\n",
    "\n",
    "    columns = ['digit', 'letter'] + [str(x) for x in range(784)]\n",
    "    aug = pd.DataFrame(new_data_set, columns=columns)\n",
    "\n",
    "    train_norm = pd.concat([ input_imgs.iloc[:,1:3], np.divide(input_imgs.iloc[:,3:],255) ],axis=1)\n",
    "    train_aug = pd.concat([train_norm,aug])\n",
    "\n",
    "    return train_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZXZDD5Hq2SY"
   },
   "outputs": [],
   "source": [
    "def train_test_gen(input_imgs):\n",
    "    train_aug = augmentation(input_imgs)\n",
    "\n",
    "    x_train = train_aug.iloc[:,2:].values.copy()\n",
    "    x_train = x_train.reshape(-1,28,28,1)\n",
    "\n",
    "    y_train = train_aug['digit']\n",
    "    y_train = to_categorical(y_train,num_classes = 10)\n",
    "\n",
    "    return train_test_split(x_train,y_train,test_size=0.1,random_state=randint(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xivSlpphyioQ"
   },
   "outputs": [],
   "source": [
    "def load_best(file_name):\n",
    "    filepath = './drive/My Drive/DACON/saved_model/' + file_name + '/'\n",
    "    time_list = []\n",
    "    for f_name in os.listdir(f\"{filepath}\"):\n",
    "        written_time = os.path.getctime(f\"{filepath}{f_name}\")\n",
    "        time_list.append((f_name, written_time))\n",
    "    sorted_file_list = sorted(time_list, key=lambda x: x[1], reverse=True)\n",
    "    best = sorted_file_list[0]\n",
    "    best_name = best[0]\n",
    "    model = load_model( filepath + best_name )\n",
    "    print('\\033[31m' + best_name + '\\033[0m')\n",
    "    print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8zxyn8pzylk"
   },
   "outputs": [],
   "source": [
    "def get_model(N):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters = N, kernel_size = (5,5),padding = 'Same', \n",
    "                    activation ='relu', input_shape = (28,28,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters = N, kernel_size = (5,5),padding = 'Same', \n",
    "                    activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "                \n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(filters = 2*N, kernel_size = (3,3),padding = 'Same', \n",
    "                    activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters = 2*N, kernel_size = (3,3),padding = 'Same', \n",
    "                    activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4*N, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hH6l9oIiX-R-"
   },
   "outputs": [],
   "source": [
    "def ensemble(input_imgs, model_list):\n",
    "    pred = []\n",
    "    L = input_imgs.shape[0]\n",
    "    label_list = np.zeros((L,10))\n",
    "    num = len(model_list)\n",
    "    for i in range(num):\n",
    "        label = model_list[i].predict_on_batch( np.array(input_imgs).reshape(-1,28,28,1).astype(np.float32) )\n",
    "        # label_list += label\n",
    "        label_list += label*(1+i*0.2)\n",
    "    for j in range(len(label_list)):\n",
    "        pred.append( np.argmax(label_list[j]) )\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhSXLtQ3YCtR"
   },
   "outputs": [],
   "source": [
    "def compare(file1,file2):\n",
    "    filepath1 = './drive/My Drive/DACON/submission/' + file1 +'.csv'\n",
    "    filepath2 = './drive/My Drive/DACON/submission/' + file2 +'.csv'\n",
    "    f1 = pd.read_csv(filepath1)\n",
    "    f2 = pd.read_csv(filepath2)\n",
    "    match = np.array( [ f1['digit']==f2['digit'] ][0] )\n",
    "    acc = len( np.where(match==True)[0] )/len(match)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InPZtCByYFG6"
   },
   "outputs": [],
   "source": [
    "def pred_acc(file_name,file_list):\n",
    "    score = []\n",
    "    for i in range( len(file_list) ):\n",
    "        acc = compare(file_name, file_list[i])\n",
    "        score.append(acc)\n",
    "        print( 'Compared with ' + file_list[i].replace('submision_','') + ' : {}'.format(acc) )\n",
    "    #return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1VyimugoZgj"
   },
   "outputs": [],
   "source": [
    "def set_folder_path(folder_name):\n",
    "    MODEL_SAVE_FOLDER_PATH = './drive/My Drive/DACON/saved_model/'+ folder_name +'/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "    return MODEL_SAVE_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2fhX4jVbN-3"
   },
   "source": [
    "# Re-run part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9U-uR2gh1z2"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 100\n",
    "folder_name = 'model_boosting'\n",
    "model_list = []\n",
    "acc_list = []\n",
    "\n",
    "\n",
    "iter = 10\n",
    "w=2\n",
    "\n",
    "for i in range(iter):\n",
    "\n",
    "    print('\\033[31m' + 'Boosting : {}'.format(i) +  '\\033[0m')\n",
    "\n",
    "    # define train/test data\n",
    "    train_aug = augmentation(train_copy)\n",
    "    x_train = train_aug.iloc[:,2:].values.copy()\n",
    "    x_train = x_train.reshape(-1,28,28,1)\n",
    "    y_train = train_aug['digit']\n",
    "    y_train = to_categorical(y_train,num_classes = 10)\n",
    "\n",
    "    if i>0 :\n",
    "        x_error =  x_val[error_id]\n",
    "        y_error = y_val[error_id]\n",
    "        for i in range(w): \n",
    "            x_train = np.concatenate( (x_train,x_error),axis=0 )\n",
    "        for i in range(w):\n",
    "            y_train = np.concatenate( (y_train,y_error), axis=0 )\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=randint(1,100))\n",
    "\n",
    "    # create model\n",
    "    model = get_model(64)\n",
    "    model_path = set_folder_path(folder_name) + 'model_{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "    # callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='max')\n",
    "    mcp_save = ModelCheckpoint(filepath = model_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n",
    "    # compile\n",
    "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    # fit model\n",
    "    hist = model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n",
    "                validation_data = (x_val,y_val),\n",
    "                steps_per_epoch=x_train.shape[0]// batch_size, \n",
    "                callbacks=[early_stopping,mcp_save,reduce_lr_loss])\n",
    "    model = load_best(folder_name)\n",
    "    model_list.append(model)\n",
    "    acc_list.append(hist.history['val_accuracy'][-11])\n",
    "\n",
    "    prob = model.predict_on_batch( x_val )\n",
    "    pred = []\n",
    "    val = []\n",
    "    for i in range(len(prob)):\n",
    "        pred.append( np.argmax(prob[i]) )\n",
    "        val.append( np.argmax(y_val[i]) )\n",
    "    error = [ np.array(pred)!=np.array(val) ]\n",
    "    error_id = np.where(error[0]==True)[0]\n",
    "\n",
    "    print('\\033[31m' + 'val_accuracy : {}'.format(hist.history['val_accuracy'][-11]) +  '\\033[0m')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tPawjf-AyyPQ"
   },
   "outputs": [],
   "source": [
    "x_test = np.divide(test_copy.iloc[:,2:].values,255)\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "pred = ensemble(x_test, model_list)\n",
    "data = {'id':test_copy['id'], 'digit':pred}\n",
    "submission = DataFrame(data)\n",
    "\n",
    "file_name = 'submission_cnn_boosting_10_w4'\n",
    "submission.to_csv('./drive/My Drive/DACON/submission/'+ file_name +'.csv', index=False)\n",
    "file_list = [ 'submission_84',\n",
    "             'submission_85',\n",
    "             'submission_86_xgb_ensemble',\n",
    "             'submission_87_ensembles',\n",
    "             'submission_88_ensemble_2_2_4_try3',\n",
    "             'submission_89_ensemble_2_2',\n",
    "             'submission_89',\n",
    "             'submission_91_ensembles_3+1_w1',\n",
    "             'submission_88_cnn_boosting_3_w2',\n",
    "             'submission_cnn_boosting_3_w3'\n",
    "             ]\n",
    "pred_acc(file_name,file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqKtQSmHKfvb"
   },
   "outputs": [],
   "source": [
    "compare(file_name,'submission_88_cnn_boosting_10_w4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ucylDP0-htQD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_boosting.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
