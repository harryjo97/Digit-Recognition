{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"unet+cnn.ipynb","provenance":[{"file_id":"1pJ23C0vqBmtLfTkHodPMZITVZSoNkPbH","timestamp":1598243637505}],"private_outputs":true,"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"scrolled":true,"id":"ELnwcr_th1zW","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import seaborn as sns\n","%matplotlib inline\n","\n","# np.random.seed(2)\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ReduceLROnPlateau\n","\n","sns.set(style='white', context='notebook', palette='deep')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bX6w-z38h1za","colab_type":"code","colab":{}},"source":["# pandas version\n","print('pandas',pd.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awVEMcgSh1zd","colab_type":"code","colab":{}},"source":["# define train set\n","from google.colab import drive\n","drive.mount('/content/drive')\\\n","\n","train = pd.read_csv('./drive/My Drive/DACON/data/train.csv')\n","train_copy = train.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDIiZ-SLtrgR","colab_type":"code","colab":{}},"source":["# resize the input dataframe of (28,28) images to array of (1,112,112,1) images\n","def resize(input_imgs) :\n","\n","    # zero padding\n","    img = np.zeros((input_imgs.shape[0],112,112,1))\n","    npad = ((42,42),(42,42))\n","    for i in range(img.shape[0]):\n","        tmp = np.divide(input_imgs.iloc[i].values,255)\n","        img[i] = np.pad(tmp.reshape(28,28),npad,'constant',constant_values=(0)).reshape(1,112,112,1) \n","    \n","    return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xEIZeRY8Vi3x","colab_type":"code","colab":{}},"source":["def avg_digit(input_imgs,th):\n","    \n","    img = np.zeros((10,784))\n","\n","    for i in range(10):\n","        digit_data = input_imgs[input_imgs['digit']==i]\n","        digit = digit_data.iloc[0,3:]\n","        for j in range(1,digit_data.shape[0]-1):\n","            digit += digit_data.iloc[j,3:].values \n","        digit = np.divide(digit,digit_data.shape[0])\n","        digit[digit< th]=0\n","        img[i] = digit\n","\n","    return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSCVQotWWsyn","colab_type":"code","colab":{}},"source":["a = avg_digit(train_copy)\n","plt.figure(figsize=(10,10))\n","for i in range(10):\n","    plt.subplot(1,10,1+i)\n","    plt.imshow(a[i].reshape(28,28),cmap='gray')\n","    plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"swoLq_9nYx1S","colab_type":"code","colab":{}},"source":["def avg_letter(input_imgs,th):\n","\n","    img = np.zeros((26,784))\n","\n","    for i in range(26):\n","        letter_data = input_imgs[input_imgs['letter']==chr(i+65)]\n","        letter = letter_data.iloc[0,3:]\n","        for j in range(1,letter_data.shape[0]-1):\n","            letter += letter_data.iloc[j,3:].values \n","        letter = np.divide(letter,letter_data.shape[0])\n","        letter[letter< th ]=0\n","        img[i] = letter\n","\n","    return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4LA72XuZJZj","colab_type":"code","colab":{}},"source":["a = avg_letter(train_copy)\n","plt.figure(figsize=(20,20))\n","for i in range(26):\n","    plt.subplot(1,26,i+1)\n","    plt.imshow(a[i].reshape(28,28),cmap='gray')\n","    plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"f9M2_GaVcNRS","colab":{}},"source":["# create the image label for u-net\n","def img_label(input_imgs) :\n","\n","    label_array = np.zeros((10,26,784))\n","\n","    # try1. avg of images with same digit and letter\n","    \"\"\"for i in range(10): # for each digit\n","        for j in range(26): # for each letter\n","            label_set = input_imgs[(input_imgs['digit']==i) & (input_imgs['letter']==chr(j+65))]\n","            label = label_set.iloc[0,3:]\n","            for k in range(1,label_set.shape[0]-1):\n","                label += label_set.iloc[k,3:] \n","            # normalize\n","            label = np.divide(label,label_set.shape[0])\n","            label[label < 35]=0\n","            label_array[i][j] = label\"\"\"\n","\n","    # trsy2. avg of digit + avg of letter\n","    \"\"\"digit = avg_digit(input_imgs)\n","    letter = avg_letter(input_imgs)\n","\n","    for i in range(10): # for each digit\n","        for j in range(26): # for each letter\n","            label_array[i][j] = np.divide( (digit[i]+letter[j]) ,2)\n","    \"\"\"\n","\n","    \"\"\"# try3. overlap avg of digit and letter\n","    digit = avg_digit(input_imgs,80)\n","    letter = avg_letter(input_imgs,40)\n","\n","    for i in range(10): # for each digit\n","        for j in range(26): # for each letter\n","            ov = train_copy.copy().iloc[0,3:]\n","            for k in range(784):\n","                if letter[j][k]==0 : label_array[i][j][k] = letter[j][k]\n","                else : label_array[i][j][k] = min((digit[i][k]+letter[j][k]),255)\"\"\"\n","\n","    # try4. digit only survive\n","    digit = avg_digit(input_imgs,80)\n","    letter = avg_letter(input_imgs,40)\n","\n","    for i in range(10): # for each digit\n","        for j in range(26): # for each letter\n","            ov = train_copy.copy().iloc[0,3:]\n","            for k in range(784):\n","                if (letter[j][k]>0) & (digit[i][k]>0) : label_array[i][j][k] = min((digit[i][k]+letter[j][k]),255)\n","                else : label_array[i][j][k] = 0\n","\n","    return label_array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFiIYVHTcTqT","colab_type":"code","colab":{}},"source":["a = img_label(train_copy)\n","plt.imshow(a[2][2].reshape(28,28),cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1NNiXFmYh1zy","colab_type":"code","colab":{}},"source":["# prepare data for unet\n","x_train = resize(train_copy.iloc[:,3:])\n","\n","y_label = img_label(train_copy) \n","y_28 = train_copy.iloc[:,3:].copy()\n","\n","for i in range(train_copy.shape[0]):\n","    y_28.iloc[i,:] = y_label[ train_copy.iloc[i,1] ][ ord(train_copy.iloc[i,2])-65 ] \n","\n","y_train = resize( y_28 )\n","\n","plt.subplot(1,2,2)\n","plt.imshow(y_train[0].reshape(112,112),cmap='gray')\n","\n","# split training and validation set\n","x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wk57niLWh1z0","colab_type":"code","colab":{}},"source":["# unet architecture\n","\n","def unet(input_size = (112,112,1)):\n","    inputs = Input(input_size)\n","    N = 16\n","    conv1 = Conv2D(N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n","    conv1 = Conv2D(N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","    conv2 = Conv2D(2*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n","    conv2 = Conv2D(2*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","    conv3 = Conv2D(4*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","    conv3 = Conv2D(4*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","    conv4 = Conv2D(8*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","    conv4 = Conv2D(8*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n","    drop4 = Dropout(0.5)(conv4)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","    conv5 = Conv2D(16*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","    conv5 = Conv2D(16*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","    drop5 = Dropout(0.5)(conv5)\n","\n","    up6 = Conv2D(8*N, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n","    merge6 = concatenate([drop4,up6], axis = 3)\n","    conv6 = Conv2D(8*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","    conv6 = Conv2D(8*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","\n","    up7 = Conv2D(4*N, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","    merge7 = concatenate([conv3,up7], axis = 3)\n","    conv7 = Conv2D(4*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","    conv7 = Conv2D(4*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","\n","    up8 = Conv2D(2*N, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","    merge8 = concatenate([conv2,up8], axis = 3)\n","    conv8 = Conv2D(2*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","    conv8 = Conv2D(2*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","\n","    up9 = Conv2D(N, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n","    merge9 = concatenate([conv1,up9], axis = 3)\n","    conv9 = Conv2D(N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","    conv9 = Conv2D(N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n","\n","    model = Model(inputs = inputs, outputs = conv10)\n","\n","    model.compile(optimizer = Adam(lr = 1e-4), loss = \"binary_crossentropy\", metrics = ['accuracy'])\n","    \n","    # model.summary()\n","\n","    # if(pretrained_weights): model.load_weights(pretrained_weights)\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55KbC-RrQsDA","colab_type":"code","colab":{}},"source":["# with data augmentation to prevent overfitting \n","\n","datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n","        zoom_range = 0.1, # Randomly zoom image \n","        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n","        horizontal_flip=False,  # randomly flip images\n","        vertical_flip=False)  # randomly flip images\n","\n","\n","datagen.fit(x_train)\n","\n","# set a learning rate annealer\n","learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n","                                            patience=3, \n","                                            verbose=1, \n","                                            factor=0.5, \n","                                            min_lr=0.00001)\n","\n","# fit the model\n","\n","epochs = 5\n","batch_size = 50\n","\n","model_u = unet()\n","history = model_u.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n","                              epochs = epochs, \n","                              validation_data = (x_val,y_val),\n","                              steps_per_epoch=x_train.shape[0]// batch_size\n","                              ,callbacks=[learning_rate_reduction]\n","                             )\n","\n","# plot the loss and accuracy curves for training and validation \n","fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EmEGg6daQ_UJ","colab_type":"code","colab":{}},"source":["# prepare data for CNN\n","x_train_ = resize(train_copy.iloc[:,3:])\n","\n","for i in range(x_train_.shape[0]):\n","    x_train_[i] = model_u.predict(x_train_[i].reshape(1,112,112,1))\n","    if i%100==0 : print(i)\n","\n","# label encoding\n","y_train_ = train_copy['digit'].values\n","y_train_ = to_categorical(y_train_,num_classes = 10)\n","\n","# split training and validation set\n","x_train_, x_val_, y_train_, y_val_ = train_test_split(x_train_,y_train_,test_size=0.1,random_state=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2CGF7rZgddF","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(15,15))\n","for i in range(3):\n","    plt.subplot(1,3,i+1)\n","    plt.imshow(x_train_[i].reshape(112,112),cmap='gray')\n","    plt.title('digit:{}'.format(np.where(y_train_[i]==1)[0][0]),fontsize=20)\n","\n","plt.figure(figsize=(15,15))\n","for i in range(3):\n","    plt.subplot(1,3,i+1)\n","    plt.imshow(x_train_[i+3].reshape(112,112),cmap='gray')\n","    plt.title('digit:{}'.format(np.where(y_train_[i+3]==1)[0][0]),fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9U-uR2gh1z2","colab_type":"code","colab":{}},"source":["# set the CNN model \n","# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n","\n","model = Sequential()\n","\n","model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (112,112,1)))\n","model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n","model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(256, activation = \"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(10, activation = \"softmax\"))\n","\n","# define the optimizer\n","optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n","\n","# compile the model\n","model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","# set a learning rate annealer\n","learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n","                                            patience=3, \n","                                            verbose=1, \n","                                            factor=0.5, \n","                                            min_lr=0.00001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GI0WNOych1z9","colab_type":"code","colab":{}},"source":["# with data augmentation to prevent overfitting \n","\n","datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n","        zoom_range = 0.1, # Randomly zoom image \n","        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n","        horizontal_flip=False,  # randomly flip images\n","        vertical_flip=False)  # randomly flip images\n","\n","\n","datagen.fit(x_train_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"bkG5N-XWh1z_","colab_type":"code","colab":{}},"source":["epochs = 10\n","batch_size = 50\n","\n","# fit the model\n","history = model.fit_generator(datagen.flow(x_train_,y_train_, batch_size=batch_size), epochs = epochs, validation_data = (x_val_,y_val_),\n","                              steps_per_epoch=x_train.shape[0]// batch_size, callbacks=[learning_rate_reduction])\n","\n","# plot the loss and accuracy curves for training and validation \n","fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wta3EG4th10B","colab_type":"code","colab":{}},"source":["# Predict the values from the validation dataset\n","y_pred = model.predict(x_val_)\n","# Convert predictions classes to one hot vectors \n","y_pred_classes = np.argmax(y_pred,axis = 1) \n","# Convert validation observations to one hot vectors\n","y_true = np.argmax(y_val_,axis = 1) \n","\n","\n","# Errors are difference between predicted labels and true labels\n","errors = (y_pred_classes - y_true != 0)\n","\n","y_pred_classes_errors = y_pred_classes[errors]\n","y_pred_errors = y_pred[errors]\n","y_true_errors = y_true[errors]\n","x_val_errors = x_val[errors]\n","\n","def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n","    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n","    n = 0\n","    nrows = 2\n","    ncols = 3\n","    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n","    for row in range(nrows):\n","        for col in range(ncols):\n","            error = errors_index[n]\n","            ax[row,col].imshow((img_errors[error]).reshape((112,112)),cmap='gray')\n","            ax[row,col].axis('off')\n","            ax[row,col].set_title(\"Predicted:{} / True:{}\".format(pred_errors[error],obs_errors[error]),fontsize=10)\n","            n += 1\n","\n","# Probabilities of the wrong predicted numbers\n","y_pred_errors_prob = np.max(y_pred_errors,axis = 1)\n","\n","# Predicted probabilities of the true values in the error set\n","true_prob_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis=1))\n","\n","# Difference between the probability of the predicted label and the true label\n","delta_pred_true_errors = y_pred_errors_prob - true_prob_errors\n","\n","# Sorted list of the delta prob errors\n","sorted_dela_errors = np.argsort(delta_pred_true_errors)\n","\n","# Top 6 errors \n","most_important_errors = sorted_dela_errors[-6:]\n","\n","# Show the top 6 errors\n","display_errors(most_important_errors, x_val_errors, y_pred_classes_errors, y_true_errors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1wPW1Mhh10F","colab_type":"code","colab":{}},"source":["\n","\"\"\"history = model.fit(x_train_, y_train_, \n","                    batch_size = batch_size, epochs = epochs, \n","                    validation_data = (x_val_, y_val_))\n","\n","# Plot the loss and accuracy curves for training and validation \n","fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)\"\"\"\n"],"execution_count":null,"outputs":[]}]}