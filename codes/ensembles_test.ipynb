{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELnwcr_th1zW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import *\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "import xgboost as xgb \n",
    "from xgboost import plot_importance , XGBClassifier, DMatrix\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awVEMcgSh1zd"
   },
   "outputs": [],
   "source": [
    "# define train set\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "train = pd.read_csv('./drive/My Drive/DACON/data_file/train.csv')\n",
    "test = pd.read_csv('./drive/My Drive/DACON/data_file/test.csv')\n",
    "train_copy = train.copy()\n",
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2e7FkuFoE7M"
   },
   "outputs": [],
   "source": [
    "rot_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=45, \n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0,     \n",
    "    zoom_range=0,      \n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant', \n",
    "    cval=0.0,            \n",
    "    horizontal_flip=False, \n",
    "    vertical_flip=False,   \n",
    "    rescale=1./255, \n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0, \n",
    "    dtype=None\n",
    ")\n",
    "\n",
    "trans_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0, \n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=None,\n",
    "    shear_range=0,     \n",
    "    zoom_range=0,      \n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant', \n",
    "    cval=0.0,             \n",
    "    horizontal_flip=False, \n",
    "    vertical_flip=False,   \n",
    "    rescale=1./255, \n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0, \n",
    "    dtype=None\n",
    ")\n",
    "\n",
    "shear_zoom_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0, \n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.2,     \n",
    "    zoom_range=0.2,      \n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant', \n",
    "    cval=0.0,             \n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,   \n",
    "    rescale=1./255, \n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0, \n",
    "    dtype=None\n",
    ")\n",
    "\n",
    "flip_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0, \n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0,     \n",
    "    zoom_range=0,      \n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant', \n",
    "    cval=0.0,             \n",
    "    horizontal_flip=True, \n",
    "    vertical_flip=True,   \n",
    "    rescale=1./255, # Rescale\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0, \n",
    "    dtype=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "me8v8S6Aq1R5"
   },
   "outputs": [],
   "source": [
    "def augmentation( input_imgs, aug_size ):\n",
    "    df = input_imgs\n",
    "    new_data_set = []\n",
    "    num_of_training_set = df.shape[0]\n",
    "\n",
    "    for i in range(num_of_training_set//2):\n",
    "        rand_1 = np.random.randint(num_of_training_set)\n",
    "        rand_2 = np.random.randint(num_of_training_set)\n",
    "        rand_3 = np.random.randint(num_of_training_set)\n",
    "        rand_4 = np.random.randint(num_of_training_set)\n",
    "    \n",
    "        for j in range( aug_size ):\n",
    "            # rotation\n",
    "            _rot = rot_gen.flow( np.array(df.iloc[rand_1,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n",
    "            new_data_set += [[\n",
    "                df.iloc[rand_1,1],\n",
    "                df.iloc[rand_1,2],\n",
    "            ] + list(_rot)]\n",
    "            # translation\n",
    "            _trans = trans_gen.flow( np.array(df.iloc[rand_2,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n",
    "            new_data_set += [[\n",
    "                df.iloc[rand_2,1],\n",
    "                df.iloc[rand_2,2],\n",
    "            ] + list(_trans)]\n",
    "            # shear / zoom\n",
    "            _shear = shear_zoom_gen.flow( np.array(df.iloc[rand_3,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n",
    "            new_data_set += [[\n",
    "                df.iloc[rand_3,1],\n",
    "                df.iloc[rand_3,2],\n",
    "            ] + list(_shear)]\n",
    "            # flip\n",
    "            _flip = flip_gen.flow( np.array(df.iloc[rand_4,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n",
    "            new_data_set += [[\n",
    "                df.iloc[rand_4,1],\n",
    "                df.iloc[rand_4,2],\n",
    "            ] + list(_flip)]\n",
    "\n",
    "    columns = ['digit', 'letter'] + [str(x) for x in range(784)]\n",
    "    aug = pd.DataFrame(new_data_set, columns=columns)\n",
    "\n",
    "    train_norm = pd.concat([ input_imgs.iloc[:,1:3], np.divide(input_imgs.iloc[:,3:],255) ],axis=1)\n",
    "    train_aug = pd.concat([train_norm,aug])\n",
    "\n",
    "    return train_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZXZDD5Hq2SY"
   },
   "outputs": [],
   "source": [
    "def train_test_gen(input_imgs, aug_size):\n",
    "    train_aug = augmentation(input_imgs, aug_size)\n",
    "\n",
    "    x_train = train_aug.iloc[:,2:].values.copy()\n",
    "    x_train = x_train.reshape(-1,28,28,1)\n",
    "\n",
    "    y_train = train_aug['digit']\n",
    "    y_train = to_categorical(y_train,num_classes = 10)\n",
    "\n",
    "    return train_test_split(x_train,y_train,test_size=0.1,random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xivSlpphyioQ"
   },
   "outputs": [],
   "source": [
    "def load_best(file_name):\n",
    "    filepath = './drive/My Drive/DACON/saved_model/' + file_name + '/'\n",
    "    time_list = []\n",
    "    for f_name in os.listdir(f\"{filepath}\"):\n",
    "        written_time = os.path.getctime(f\"{filepath}{f_name}\")\n",
    "        time_list.append((f_name, written_time))\n",
    "    sorted_file_list = sorted(time_list, key=lambda x: x[1], reverse=True)\n",
    "    best = sorted_file_list[0]\n",
    "    best_name = best[0]\n",
    "    model = load_model( filepath + best_name )\n",
    "    print('\\033[31m' + best_name + '\\033[0m')\n",
    "    print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8N9Nwclq3oL"
   },
   "outputs": [],
   "source": [
    "def set_filepath(file_name):\n",
    "    MODEL_SAVE_FOLDER_PATH = './drive/My Drive/DACON/saved_model/' + file_name + '/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "    return MODEL_SAVE_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8zxyn8pzylk"
   },
   "outputs": [],
   "source": [
    "def get_model(N):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters = N, kernel_size = (5,5),padding = 'Same', \n",
    "                    activation ='relu', input_shape = (28,28,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters = N, kernel_size = (5,5),padding = 'Same', \n",
    "                    activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "                \n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(filters = 2*N, kernel_size = (3,3),padding = 'Same', \n",
    "                    activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters = 2*N, kernel_size = (3,3),padding = 'Same', \n",
    "                    activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4*N, activation = \"relu\", name='my_dense'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZUHV_T2iQ2c"
   },
   "outputs": [],
   "source": [
    "def compare(file1,file2):\n",
    "    filepath1 = './drive/My Drive/DACON/submission/' + file1 +'.csv'\n",
    "    filepath2 = './drive/My Drive/DACON/submission/' + file2 +'.csv'\n",
    "    f1 = pd.read_csv(filepath1)\n",
    "    f2 = pd.read_csv(filepath2)\n",
    "    match = np.array( [ f1['digit']==f2['digit'] ][0] )\n",
    "    acc = len( np.where(match==True)[0] )/len(match)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eyaiT-_2iRtE"
   },
   "outputs": [],
   "source": [
    "def pred_acc(file_name,file_list):\n",
    "    score = []\n",
    "    for i in range( len(file_list) ):\n",
    "        acc = compare(file_name, file_list[i])\n",
    "        score.append(acc)\n",
    "        print( 'Compared with ' + file_list[i].replace('submision_','') + ' : {}'.format(acc) )\n",
    "    #return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqtGXmhcokz0"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rCrswRMTHgD"
   },
   "outputs": [],
   "source": [
    "def train_cnn(cnn_num, N, aug_size):\n",
    "    cnn_model_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(cnn_num):\n",
    "\n",
    "        model = get_model(N)\n",
    "        file_name = 'model_ensembles'\n",
    "        model_path = set_filepath(file_name) + 'cnn_{}'.format(i) + '_{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "        # callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='max')\n",
    "        mcp_save = ModelCheckpoint(filepath = model_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n",
    "        # compile\n",
    "        optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "        model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        # fit model\n",
    "        x_train, x_val, y_train, y_val = train_test_gen(train_copy, aug_size)\n",
    "\n",
    "        hist = model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n",
    "                    validation_data = (x_val,y_val),\n",
    "                    steps_per_epoch=x_train.shape[0]// batch_size, \n",
    "                    callbacks=[early_stopping,mcp_save,reduce_lr_loss])\n",
    "        model = load_best(file_name)\n",
    "\n",
    "        cnn_model_list.append(model)\n",
    "        acc_list.append(hist.history['val_accuracy'][-11])\n",
    "\n",
    "    return cnn_model_list, acc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moUYuzYyU0-7"
   },
   "outputs": [],
   "source": [
    "def train_xgb(xgb_num, N, aug_size):\n",
    "    inter_model_list = []\n",
    "    layer_name='my_dense'\n",
    "\n",
    "    for i in range(xgb_num):\n",
    "\n",
    "        model = get_model(N)\n",
    "        file_name = 'model_ensembles'\n",
    "        model_path = set_filepath(file_name) + 'xgb_{}'.format(i) + '_{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "        # callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='max')\n",
    "        mcp_save = ModelCheckpoint(filepath = model_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n",
    "\n",
    "        optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "        model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        # fit model\n",
    "        x_train, x_val, y_train, y_val = train_test_gen(train_copy, aug_size)\n",
    "\n",
    "        hist = model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n",
    "                    validation_data = (x_val,y_val),\n",
    "                    steps_per_epoch=x_train.shape[0]// batch_size, \n",
    "                    callbacks=[early_stopping,mcp_save,reduce_lr_loss])\n",
    "        model = load_best(file_name)\n",
    "        \n",
    "        inter_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "        inter_model_list.append(inter_model)\n",
    "\n",
    "    train_aug = augmentation(train_copy, aug_size)\n",
    "    inter_train = train_aug.iloc[:,2:].values.reshape(-1,28,28,1)\n",
    "    cnn_output_list = []\n",
    "    cnn_val_list = []\n",
    "    for i in range(xgb_num):\n",
    "        cnn_output = inter_model_list[i].predict( inter_train ) \n",
    "        cnn_output = pd.DataFrame( data=cnn_output )\n",
    "        cnn_output_list.append(cnn_output)\n",
    "        cnn_val = train_aug['digit']\n",
    "        cnn_val_list.append(cnn_val)\n",
    "\n",
    "    xgb_model_list = []\n",
    "    for i in range(xgb_num):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(cnn_output_list[i], cnn_val_list[i],test_size=0.1,random_state=25)\n",
    "\n",
    "        xgb_model = XGBClassifier(max_depth=5, num_class=10, objective='multi:softprob', booster='gbtree', n_estimators=300, learning_rate=0.2 )\n",
    "        xgb_model.fit( x_train, y_train, eval_set=[(x_val, y_val)], eval_metric='mlogloss', early_stopping_rounds=5)\n",
    "\n",
    "        xgb_model_list.append(xgb_model)\n",
    "\n",
    "    return xgb_model_list, inter_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbaMoSyggqX7"
   },
   "outputs": [],
   "source": [
    "cnn_num = 10\n",
    "xgb_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGYmtEZMWrhC"
   },
   "outputs": [],
   "source": [
    "cnn_model_list, acc_list = train_cnn(cnn_num, 64, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0L6p8LiztPXf"
   },
   "outputs": [],
   "source": [
    "xgb_model_list, inter_model_list = train_xgb(xgb_num,128,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjvdyGKDzDE8"
   },
   "outputs": [],
   "source": [
    "def ensemble(input_imgs,cnn_model_list, inter_model_list, xgb_model_list, w_cnn, w_xgb):\n",
    "    pred = []\n",
    "    L = input_imgs.shape[0]\n",
    "    label_list = np.zeros((L,10))\n",
    "    for i in range(len(cnn_model_list)):\n",
    "        label = cnn_model_list[i].predict( np.array(input_imgs).reshape(-1,28,28,1).astype(np.float32) )\n",
    "        label_list += label*w_cnn\n",
    "\n",
    "    for i in range(len(xgb_model_list)):\n",
    "        cnn_output = inter_model_list[i].predict( np.array(input_imgs).reshape(-1,28,28,1).astype(np.float32) )\n",
    "        cnn_output = DataFrame(cnn_output)\n",
    "        label = xgb_model_list[i].predict_proba( cnn_output )\n",
    "        label_list += label*w_xgb\n",
    "        \n",
    "    for j in range(len(label_list)):\n",
    "        pred.append( np.argmax(label_list[j]) )\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LFpmt2loWr6U"
   },
   "outputs": [],
   "source": [
    "x_test = np.divide(test_copy.iloc[:,2:].values,255)\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "pred = ensemble(x_test, cnn_model_list, inter_model_list, xgb_model_list)\n",
    "data = {'id':test_copy['id'], 'digit':pred}\n",
    "submission = DataFrame(data)\n",
    "file_name = 'submission_ensembles_6+2_bn_07'\n",
    "submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n",
    "print(pred[:10])\n",
    "print()\n",
    "file_list = [ 'submission_84',\n",
    "             'submission_85',\n",
    "             'submission_86_xgb_ensemble',\n",
    "             'submission_87_ensembles',\n",
    "             'submission_88_ensemble_2_2_4_try3',\n",
    "             'submission_89_ensemble_2_2',\n",
    "             'submission_89',\n",
    "             'submission_91_ensembles_3+1_w1',\n",
    "             'submission_91_ensembles_6+2_bn_08'\n",
    "             ]\n",
    "pred_acc(file_name,file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-xJsDP8hB1L"
   },
   "outputs": [],
   "source": [
    "def to_data(input, cnn_model_list, inter_model_list, xgb_model_list):\n",
    "\n",
    "    pred_data = []\n",
    "    a = len(cnn_model_list)\n",
    "    b = len(xgb_model_list)\n",
    "    for i in range(a):\n",
    "           pred_data.append( cnn_model_list[i].predict(input) )\n",
    "    for i in range(b):\n",
    "        cnn_output = inter_model_list[i].predict( input )\n",
    "        cnn_output = DataFrame(cnn_output)\n",
    "        pred_data.append( xgb_model_list[i].predict_proba( cnn_output ) )\n",
    "\n",
    "    data = pred_data[0]\n",
    "    for i in range(1,a+b):\n",
    "        data = np.concatenate((data, pred_data[i]),axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5TKd8eByAO7"
   },
   "outputs": [],
   "source": [
    "def linear_reg(aug_size, cnn_model_list, inter_model_list, xgb_model_list):\n",
    "    \n",
    "    train_aug = augmentation(train_copy, aug_size)\n",
    "    input = train_aug.iloc[:,2:].values.copy().reshape(-1,28,28,1)\n",
    "    data = to_data(input, cnn_model_list, inter_model_list, xgb_model_list)\n",
    "    data_val = train_aug['digit']\n",
    "    data_val = to_categorical(data_val, 10)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(data, data_val, test_size = 0.1)\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(x_train, y_train)\n",
    "    print('Coefficients: {}', format(linear_model.coef_[0]))\n",
    "    print(\"RSS: {}\".format( np.mean((linear_model.predict(x_val) - y_val) ** 2) ))\n",
    "\n",
    "    return linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OF-XGJDX6Cx3"
   },
   "outputs": [],
   "source": [
    "train_aug_check = augmentation(train_copy,3)\n",
    "x_check = train_aug_check.iloc[:,2:].values\n",
    "x_check = x_check.reshape(-1,28,28,1)\n",
    "y_check = train_aug_check['digit']\n",
    "\n",
    "data_check = to_data(x_check, cnn_model_list, inter_model_list, xgb_model_list)\n",
    "\n",
    "linear_model = linear_reg(3, cnn_model_list, inter_model_list, xgb_model_list)\n",
    "pred = linear_model.predict( data_check )\n",
    "pred[:10]\n",
    "pred_check = []\n",
    "for i in range(len(pred)):\n",
    "    pred_check.append( np.argmax(pred[i]) )\n",
    "\n",
    "s = [np.array(pred_check)==np.array(y_check)]\n",
    "t = np.where(s[0]==True)\n",
    "acc = len(t[0])/len(s[0])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEfZohec7gYa"
   },
   "outputs": [],
   "source": [
    "x_test = np.divide(test_copy.iloc[:,2:].values,255)\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "data_test = to_data(x_test, cnn_model_list, inter_model_list, xgb_model_list)\n",
    "#linear_model = linear_reg(3, cnn_model_list, inter_model_list, xgb_model_list)\n",
    "\n",
    "pred = linear_model.predict( data_test )\n",
    "pred_test = []\n",
    "for i in range(len(pred)):\n",
    "    pred_test.append( np.argmax(pred[i]) )\n",
    "pred_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGqIBkvSTkoS"
   },
   "outputs": [],
   "source": [
    "data = {'id':test_copy['id'], 'digit':pred_test}\n",
    "submission = DataFrame(data)\n",
    "file_name = 'submission_ensembles_10+5_bn_linearreg'\n",
    "submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n",
    "file_list = [ 'submission_84',\n",
    "             'submission_85',\n",
    "             'submission_86_xgb_ensemble',\n",
    "             'submission_87_ensembles',\n",
    "             'submission_88_ensemble_2_2_4_try3',\n",
    "             'submission_89_ensemble_2_2',\n",
    "             'submission_89',\n",
    "             'submission_90_ensembles_6+2_bn_08_retry',\n",
    "             'submission_90_pretrain_using_test_layer_4_3ensemble',\n",
    "             'submission_90_ensembles_6+2_bn_linearreg',\n",
    "             'submission_91_ensembles_3+1_w1',\n",
    "             'submission_91_ensembles_6+2_bn_08'\n",
    "             ]\n",
    "pred_acc(file_name,file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVD60oJjOD67"
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_FOLDER_PATH = './drive/My Drive/DACON/saved_model/' + 'model_ensembles_10+5' + '/'\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "for i in range(len(cnn_model_list)):\n",
    "    cnn_model_list[i].save(MODEL_SAVE_FOLDER_PATH  + 'cnn_model_list_{}.hdf5'.format(i))\n",
    "for i in range(len(inter_model_list)):\n",
    "    inter_model_list[i].save(MODEL_SAVE_FOLDER_PATH  + 'inter_model_list_{}.hdf5'.format(i))\n",
    "for i in range(len(xgb_model_list)):\n",
    "    joblib.dump(xgb_model_list[i], MODEL_SAVE_FOLDER_PATH  + 'xgb_model_list_{}.hdf5'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rdo3NbpZTlYN"
   },
   "outputs": [],
   "source": [
    "for i in range(len(xgb_model_list)):\n",
    "    joblib.dump(xgb_model_list[i], MODEL_SAVE_FOLDER_PATH  + 'xgb_model_list_{}.dat'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cocXaTuWQWBt"
   },
   "outputs": [],
   "source": [
    "filepath = './drive/My Drive/DACON/saved_model/model_ensembles_10+5/'\n",
    "time_list = []\n",
    "for f_name in os.listdir(f\"{filepath}\"):\n",
    "    written_time = os.path.getctime(f\"{filepath}{f_name}\")\n",
    "    time_list.append((f_name, written_time))\n",
    "sorted_file_list = sorted(time_list, key=lambda x: x[1], reverse=False)\n",
    "models = sorted_file_list\n",
    "cnn_model_list_load = []\n",
    "inter_model_list_load = []\n",
    "xgb_model_list_load = []\n",
    "for i in range(cnn_num):\n",
    "    model = load_model( filepath + models[i][0] )\n",
    "    cnn_model_list_load.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GrisW4DUh_y"
   },
   "outputs": [],
   "source": [
    "for i in range(xgb_num):\n",
    "    model = load_model( filepath + models[i+cnn_num][0] )\n",
    "    inter_model_list_load.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mO_DXkkUYj8"
   },
   "outputs": [],
   "source": [
    "for i in range(xgb_num):\n",
    "    model = joblib.load( filepath + models[i+cnn_num+xgb_num][0] )\n",
    "    xgb_model_list_load.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUTv8lrEUwWC"
   },
   "outputs": [],
   "source": [
    "cnn_model_list_load[0] == cnn_model_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpVQiN88VtF1"
   },
   "outputs": [],
   "source": [
    "x_test = np.divide(test_copy.iloc[:,2:].values,255)\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "data_test_load = to_data(x_test, cnn_model_list_load, inter_model_list_load, xgb_model_list_load)\n",
    "#linear_model = linear_reg(3, cnn_model_list, inter_model_list, xgb_model_list)\n",
    "\n",
    "pred_load = linear_model.predict( data_test_load )\n",
    "pred_test_load = []\n",
    "for i in range(len(pred_load)):\n",
    "    pred_test_load.append( np.argmax(pred_load[i]) )\n",
    "pred_test_load[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3lE055VDOs7p"
   },
   "source": [
    "# Load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQ96cncxJBIj"
   },
   "outputs": [],
   "source": [
    "filepath = './drive/My Drive/DACON/saved_model/model_ensembles_6+2/'\n",
    "time_list = []\n",
    "for f_name in os.listdir(f\"{filepath}\"):\n",
    "    written_time = os.path.getctime(f\"{filepath}{f_name}\")\n",
    "    time_list.append((f_name, written_time))\n",
    "sorted_file_list = sorted(time_list, key=lambda x: x[1], reverse=False)\n",
    "models = sorted_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-J3xdGFTv3J"
   },
   "outputs": [],
   "source": [
    "model_list = []\n",
    "for i in range(len(models)):\n",
    "    model = load_model( filepath + models[i][0] )\n",
    "    model_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7vgTzgAKXZp"
   },
   "outputs": [],
   "source": [
    "inter_model_list = []\n",
    "for i in range(2):\n",
    "    model = model_list[i+6]\n",
    "    inter_model = Model(inputs=model.input, outputs=model.get_layer('my_dense').output)\n",
    "    inter_model_list.append(inter_model)\n",
    "\n",
    "train_aug = augmentation(train_copy, 3)\n",
    "inter_train = train_aug.iloc[:,2:].values.reshape(-1,28,28,1)\n",
    "cnn_output_list = []\n",
    "cnn_val_list = []\n",
    "for i in range(2):\n",
    "    cnn_output = inter_model_list[i].predict( inter_train ) \n",
    "    cnn_output = pd.DataFrame( data=cnn_output )\n",
    "    cnn_output_list.append(cnn_output)\n",
    "    cnn_val = train_aug['digit']\n",
    "    cnn_val_list.append(cnn_val)\n",
    "\n",
    "xgb_model_list = []\n",
    "for i in range(2):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(cnn_output_list[i], cnn_val_list[i],test_size=0.1,random_state=25)\n",
    "\n",
    "    xgb_model = XGBClassifier(max_depth=5, num_class=10, objective='multi:softprob', booster='gbtree', n_estimators=300, learning_rate=0.2 )\n",
    "    xgb_model.fit( x_train, y_train, eval_set=[(x_val, y_val)], eval_metric='mlogloss', early_stopping_rounds=5)\n",
    "\n",
    "    xgb_model_list.append(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6iZgrJTOnFd"
   },
   "outputs": [],
   "source": [
    "cnn_model_list = model_list[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlwjBxyyLCTg"
   },
   "outputs": [],
   "source": [
    "train_aug = augmentation(train_copy,2)\n",
    "x_train = train_aug.iloc[:,2:].values.reshape(-1,28,28,1)\n",
    "y_train = train_aug['digit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJfYT-v2Mia9"
   },
   "outputs": [],
   "source": [
    "pred = ensemble(x_train, cnn_model_list, inter_model_list, xgb_model_list, 1, 3.8)\n",
    "s = [np.array(pred)==np.array(y_train)]\n",
    "t = np.where(s[0]==True)\n",
    "acc = len(t[0])/len(s[0])\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fkPISfccPXqz"
   },
   "outputs": [],
   "source": [
    "x_test = np.divide(test_copy.iloc[:,2:].values,255)\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "pred = ensemble(x_test, cnn_model_list, inter_model_list, xgb_model_list, 1.0, 3.85)\n",
    "data = {'id':test_copy['id'], 'digit':pred}\n",
    "submission = DataFrame(data)\n",
    "file_name = 'submission_ensembles_6+2_bn_08_385'\n",
    "submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n",
    "print(pred[:10])\n",
    "print()\n",
    "file_list = [ 'submission_84',\n",
    "             'submission_85',\n",
    "             'submission_86_xgb_ensemble',\n",
    "             'submission_87_ensembles',\n",
    "             'submission_88_ensemble_2_2_4_try3',\n",
    "             'submission_89_ensemble_2_2',\n",
    "             'submission_89',\n",
    "             'submission_91_ensembles_3+1_w1',\n",
    "             'submission_91_ensembles_6+2_bn_08'\n",
    "             ]\n",
    "pred_acc(file_name,file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NEqoiXpOV5Cz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ensembles_test.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
