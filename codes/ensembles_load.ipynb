{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"ensembles_load.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"scrolled":true,"id":"ELnwcr_th1zW","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from pandas import DataFrame\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import seaborn as sns\n","sns.set(style='white', context='notebook', palette='deep')\n","%matplotlib inline\n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import *\n","from sklearn.linear_model import *\n","import itertools\n","\n","from keras.utils.np_utils import to_categorical \n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.callbacks import *\n","from keras.preprocessing.image import ImageDataGenerator\n","import os\n","\n","import xgboost as xgb \n","from xgboost import plot_importance , XGBClassifier, DMatrix\n","import joblib\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awVEMcgSh1zd","colab_type":"code","colab":{}},"source":["# define train set\n","from google.colab import drive\n","drive.mount('/content/drive')\n","train = pd.read_csv('./drive/My Drive/DACON/data_file/train.csv')\n","test = pd.read_csv('./drive/My Drive/DACON/data_file/test.csv')\n","test_pred = pd.read_csv('./drive/My Drive/DACON//submission/submission_91_ensembles_6+2_bn_08.csv')\n","train_copy = train.copy()\n","test_copy = test.copy()\n","test_pred_copy = test_pred.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m2e7FkuFoE7M","colab_type":"code","colab":{}},"source":["rot_gen = ImageDataGenerator(\n","    featurewise_center=False,\n","    samplewise_center=False,\n","    featurewise_std_normalization=False,\n","    samplewise_std_normalization=False,\n","    zca_whitening=False,\n","    zca_epsilon=1e-06,\n","    rotation_range=45, \n","    width_shift_range=0.0,\n","    height_shift_range=0.0,\n","    brightness_range=None,\n","    shear_range=0,     \n","    zoom_range=0,      \n","    channel_shift_range=0.0,\n","    fill_mode='constant', \n","    cval=0.0,            \n","    horizontal_flip=False, \n","    vertical_flip=False,   \n","    rescale=1./255, \n","    preprocessing_function=None,\n","    data_format=None,\n","    validation_split=0, \n","    dtype=None\n",")\n","\n","trans_gen = ImageDataGenerator(\n","    featurewise_center=False,\n","    samplewise_center=False,\n","    featurewise_std_normalization=False,\n","    samplewise_std_normalization=False,\n","    zca_whitening=False,\n","    zca_epsilon=1e-06,\n","    rotation_range=0, \n","    width_shift_range=0.2, \n","    height_shift_range=0.2,\n","    brightness_range=None,\n","    shear_range=0,     \n","    zoom_range=0,      \n","    channel_shift_range=0.0,\n","    fill_mode='constant', \n","    cval=0.0,             \n","    horizontal_flip=False, \n","    vertical_flip=False,   \n","    rescale=1./255, \n","    preprocessing_function=None,\n","    data_format=None,\n","    validation_split=0, \n","    dtype=None\n",")\n","\n","shear_zoom_gen = ImageDataGenerator(\n","    featurewise_center=False,\n","    samplewise_center=False,\n","    featurewise_std_normalization=False,\n","    samplewise_std_normalization=False,\n","    zca_whitening=False,\n","    zca_epsilon=1e-06,\n","    rotation_range=0, \n","    width_shift_range=0.0,\n","    height_shift_range=0.0,\n","    brightness_range=None,\n","    shear_range=0.2,     \n","    zoom_range=0.2,      \n","    channel_shift_range=0.0,\n","    fill_mode='constant', \n","    cval=0.0,             \n","    horizontal_flip=False,\n","    vertical_flip=False,   \n","    rescale=1./255, \n","    preprocessing_function=None,\n","    data_format=None,\n","    validation_split=0, \n","    dtype=None\n",")\n","\n","flip_gen = ImageDataGenerator(\n","    featurewise_center=False,\n","    samplewise_center=False,\n","    featurewise_std_normalization=False,\n","    samplewise_std_normalization=False,\n","    zca_whitening=False,\n","    zca_epsilon=1e-06,\n","    rotation_range=0, \n","    width_shift_range=0.0,\n","    height_shift_range=0.0,\n","    brightness_range=None,\n","    shear_range=0,     \n","    zoom_range=0,      \n","    channel_shift_range=0.0,\n","    fill_mode='constant', \n","    cval=0.0,             \n","    horizontal_flip=True, \n","    vertical_flip=True,   \n","    rescale=1./255, # Rescale\n","    preprocessing_function=None,\n","    data_format=None,\n","    validation_split=0, \n","    dtype=None\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"me8v8S6Aq1R5","colab_type":"code","colab":{}},"source":["def augmentation( input_imgs, aug_size ):\n","    df = input_imgs\n","    new_data_set = []\n","    num_of_training_set = df.shape[0]\n","\n","    for i in range(num_of_training_set//2):\n","        rand_1 = np.random.randint(num_of_training_set)\n","        rand_2 = np.random.randint(num_of_training_set)\n","        rand_3 = np.random.randint(num_of_training_set)\n","        rand_4 = np.random.randint(num_of_training_set)\n","    \n","        for j in range( aug_size ):\n","            # rotation\n","            _rot = rot_gen.flow( np.array(df.iloc[rand_1,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n","            new_data_set += [[\n","                df.iloc[rand_1,1],\n","                df.iloc[rand_1,2],\n","            ] + list(_rot)]\n","            # translation\n","            _trans = trans_gen.flow( np.array(df.iloc[rand_2,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n","            new_data_set += [[\n","                df.iloc[rand_2,1],\n","                df.iloc[rand_2,2],\n","            ] + list(_trans)]\n","            # shear / zoom\n","            _shear = shear_zoom_gen.flow( np.array(df.iloc[rand_3,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n","            new_data_set += [[\n","                df.iloc[rand_3,1],\n","                df.iloc[rand_3,2],\n","            ] + list(_shear)]\n","            # flip\n","            _flip = flip_gen.flow( np.array(df.iloc[rand_4,3:]).reshape(1,28,28,1) ).next().reshape(784,)\n","            new_data_set += [[\n","                df.iloc[rand_4,1],\n","                df.iloc[rand_4,2],\n","            ] + list(_flip)]\n","\n","    columns = ['digit', 'letter'] + [str(x) for x in range(784)]\n","    aug = pd.DataFrame(new_data_set, columns=columns)\n","\n","    train_norm = pd.concat([ input_imgs.iloc[:,1:3], np.divide(input_imgs.iloc[:,3:],255) ],axis=1)\n","    train_aug = pd.concat([train_norm,aug])\n","\n","    return train_aug\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZXZDD5Hq2SY","colab_type":"code","colab":{}},"source":["def train_test_gen(input_imgs, aug_size):\n","    train_aug = augmentation(input_imgs, aug_size)\n","\n","    x_train = train_aug.iloc[:,2:].values.copy()\n","    x_train = x_train.reshape(-1,28,28,1)\n","\n","    y_train = train_aug['digit']\n","    y_train = to_categorical(y_train,num_classes = 10)\n","\n","    return train_test_split(x_train,y_train,test_size=0.1,random_state=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xivSlpphyioQ","colab_type":"code","colab":{}},"source":["def load_best(file_name):\n","    filepath = './drive/My Drive/DACON/saved_model/' + file_name + '/'\n","    time_list = []\n","    for f_name in os.listdir(f\"{filepath}\"):\n","        written_time = os.path.getctime(f\"{filepath}{f_name}\")\n","        time_list.append((f_name, written_time))\n","    sorted_file_list = sorted(time_list, key=lambda x: x[1], reverse=True)\n","    best = sorted_file_list[0]\n","    best_name = best[0]\n","    model = load_model( filepath + best_name )\n","    print('\\033[31m' + best_name + '\\033[0m')\n","    print()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S8N9Nwclq3oL","colab_type":"code","colab":{}},"source":["def set_filepath(file_name):\n","    MODEL_SAVE_FOLDER_PATH = './drive/My Drive/DACON/saved_model/' + file_name + '/'\n","    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n","        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n","    \n","    return MODEL_SAVE_FOLDER_PATH"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8zxyn8pzylk","colab_type":"code","colab":{}},"source":["def get_model(N):\n","    \n","    model = Sequential()\n","\n","    model.add(Conv2D(filters = N, kernel_size = (5,5),padding = 'Same', \n","                    activation ='relu', input_shape = (28,28,1)))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(filters = N, kernel_size = (5,5),padding = 'Same', \n","                    activation ='relu'))\n","    model.add(BatchNormalization())\n","                \n","    model.add(MaxPool2D(pool_size=(2,2)))\n","    model.add(Dropout(0.25))\n","\n","\n","    model.add(Conv2D(filters = 2*N, kernel_size = (3,3),padding = 'Same', \n","                    activation ='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(filters = 2*N, kernel_size = (3,3),padding = 'Same', \n","                    activation ='relu'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n","    model.add(Dropout(0.25))\n","\n","\n","    model.add(Flatten())\n","    model.add(Dense(4*N, activation = \"relu\", name='my_dense'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","    model.add(Dense(10, activation = \"softmax\"))\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZUHV_T2iQ2c","colab_type":"code","colab":{}},"source":["def compare(file1,file2):\n","    filepath1 = './drive/My Drive/DACON/submission/' + file1 +'.csv'\n","    filepath2 = './drive/My Drive/DACON/submission/' + file2 +'.csv'\n","    f1 = pd.read_csv(filepath1)\n","    f2 = pd.read_csv(filepath2)\n","    match = np.array( [ f1['digit']==f2['digit'] ][0] )\n","    acc = len( np.where(match==True)[0] )/len(match)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eyaiT-_2iRtE","colab_type":"code","colab":{}},"source":["def pred_acc(file_name,file_list):\n","    score = []\n","    for i in range( len(file_list) ):\n","        acc = compare(file_name, file_list[i])\n","        score.append(acc)\n","        print( 'Compared with ' + file_list[i].replace('submision_','') + ' : {}'.format(acc) )\n","    #return score\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqtGXmhcokz0","colab_type":"code","colab":{}},"source":["epochs = 100\n","batch_size = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rCrswRMTHgD","colab_type":"code","colab":{}},"source":["def train_cnn(cnn_num, N, aug_size):\n","    cnn_model_list = []\n","    acc_list = []\n","\n","    for i in range(cnn_num):\n","\n","        model = get_model(N)\n","        file_name = 'model_ensembles'\n","        model_path = set_filepath(file_name) + 'cnn_{}'.format(i) + '_{val_accuracy:.4f}.hdf5'\n","\n","        # callbacks\n","        early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='max')\n","        mcp_save = ModelCheckpoint(filepath = model_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n","        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n","        # compile\n","        optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n","        model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","        # fit model\n","        x_train, x_val, y_train, y_val = train_test_gen(train_copy, aug_size)\n","\n","        hist = model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n","                    validation_data = (x_val,y_val),\n","                    steps_per_epoch=x_train.shape[0]// batch_size, \n","                    callbacks=[early_stopping,mcp_save,reduce_lr_loss])\n","        model = load_best(file_name)\n","\n","        cnn_model_list.append(model)\n","        acc_list.append(hist.history['val_accuracy'][-11])\n","\n","    return cnn_model_list, acc_list\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"moUYuzYyU0-7","colab_type":"code","colab":{}},"source":["def train_xgb(xgb_num, N, aug_size):\n","    inter_model_list = []\n","    layer_name='my_dense'\n","\n","    for i in range(xgb_num):\n","\n","        model = get_model(N)\n","        file_name = 'model_ensembles'\n","        model_path = set_filepath(file_name) + 'xgb_{}'.format(i) + '_{val_accuracy:.4f}.hdf5'\n","\n","        # callbacks\n","        early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='max')\n","        mcp_save = ModelCheckpoint(filepath = model_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n","        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n","\n","        optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n","        model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","        # fit model\n","        x_train, x_val, y_train, y_val = train_test_gen(train_copy, aug_size)\n","\n","        hist = model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n","                    validation_data = (x_val,y_val),\n","                    steps_per_epoch=x_train.shape[0]// batch_size, \n","                    callbacks=[early_stopping,mcp_save,reduce_lr_loss])\n","        model = load_best(file_name)\n","        \n","        inter_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n","        inter_model_list.append(inter_model)\n","\n","    train_aug = augmentation(train_copy, aug_size)\n","    inter_train = train_aug.iloc[:,2:].values.reshape(-1,28,28,1)\n","    cnn_output_list = []\n","    cnn_val_list = []\n","    for i in range(xgb_num):\n","        cnn_output = inter_model_list[i].predict( inter_train ) \n","        cnn_output = pd.DataFrame( data=cnn_output )\n","        cnn_output_list.append(cnn_output)\n","        cnn_val = train_aug['digit']\n","        cnn_val_list.append(cnn_val)\n","\n","    xgb_model_list = []\n","    for i in range(xgb_num):\n","        x_train, x_val, y_train, y_val = train_test_split(cnn_output_list[i], cnn_val_list[i],test_size=0.1,random_state=25)\n","\n","        xgb_model = XGBClassifier(max_depth=5, num_class=10, objective='multi:softprob', booster='gbtree', n_estimators=300, learning_rate=0.2 )\n","        xgb_model.fit( x_train, y_train, eval_set=[(x_val, y_val)], eval_metric='mlogloss', early_stopping_rounds=5)\n","\n","        xgb_model_list.append(xgb_model)\n","\n","    return xgb_model_list, inter_model_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oySSWElvXnbT","colab_type":"code","colab":{}},"source":["def ensemble(input_imgs,cnn_model_list, inter_model_list, xgb_model_list, w_cnn, w_xgb):\n","    pred = []\n","    L = input_imgs.shape[0]\n","    label_list = np.zeros((L,10))\n","    for i in range(len(cnn_model_list)):\n","        label = cnn_model_list[i].predict( np.array(input_imgs).reshape(-1,28,28,1).astype(np.float32) )\n","        label_list += label*w_cnn\n","\n","    for i in range(len(xgb_model_list)):\n","        cnn_output = inter_model_list[i].predict( np.array(input_imgs).reshape(-1,28,28,1).astype(np.float32) )\n","        cnn_output = DataFrame(cnn_output)\n","        label = xgb_model_list[i].predict_proba( cnn_output )\n","        label_list += label*w_xgb\n","        \n","    for j in range(len(label_list)):\n","        pred.append( np.argmax(label_list[j]) )\n","\n","    return pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K81gtBaZXQ_1","colab_type":"text"},"source":["# Train models"]},{"cell_type":"code","metadata":{"id":"KbaMoSyggqX7","colab_type":"code","colab":{}},"source":["cnn_num = 10\n","xgb_num = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGYmtEZMWrhC","colab_type":"code","colab":{}},"source":["cnn_model_list, acc_list = train_cnn(cnn_num, 64, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0L6p8LiztPXf","colab_type":"code","colab":{}},"source":["xgb_model_list, inter_model_list = train_xgb(xgb_num,128,3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_GWsedWWv-R","colab_type":"text"},"source":["# Save models"]},{"cell_type":"code","metadata":{"id":"eVD60oJjOD67","colab_type":"code","colab":{}},"source":["MODEL_SAVE_FOLDER_PATH = './drive/My Drive/DACON/saved_model/' + 'model_ensembles_10+5_new' + '/'\n","if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n","    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n","for i in range(len(cnn_model_list)):\n","    cnn_model_list[i].save(MODEL_SAVE_FOLDER_PATH  + 'cnn_model_list_{}.hdf5'.format(i))\n","for i in range(len(inter_model_list)):\n","    inter_model_list[i].save(MODEL_SAVE_FOLDER_PATH  + 'inter_model_list_{}.hdf5'.format(i))\n","for i in range(len(xgb_model_list)):\n","    joblib.dump(xgb_model_list[i], MODEL_SAVE_FOLDER_PATH  + 'xgb_model_list_{}.dat'.format(i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"itedvkHK4Gzm","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","pred = ensemble(x_test, cnn_model_list, inter_model_list, xgb_model_list)\n","data = {'id':test_copy['id'], 'digit':pred}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_6+2_bn_07'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","print(pred[:10])\n","print()\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_89_ensemble_2_2',\n","             'submission_89',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z25Me6j9W9QI","colab_type":"text"},"source":["# Load models for linear regression"]},{"cell_type":"code","metadata":{"id":"cocXaTuWQWBt","colab_type":"code","colab":{}},"source":["filepath = './drive/My Drive/DACON/saved_model/model_ensembles_10+5/'\n","time_list = []\n","for f_name in os.listdir(f\"{filepath}\"):\n","    written_time = os.path.getctime(f\"{filepath}{f_name}\")\n","    time_list.append((f_name, written_time))\n","sorted_file_list = sorted(time_list, key=lambda x: x[1], reverse=False)\n","models = sorted_file_list\n","cnn_model_list_load = []\n","inter_model_list_load = []\n","xgb_model_list_load = []\n","for i in range(cnn_num):\n","    model = load_model( filepath + models[i][0] )\n","    cnn_model_list_load.append(model)\n","for i in range(xgb_num):\n","    model = load_model( filepath + models[i+cnn_num][0] )\n","    inter_model_list_load.append(model)\n","for i in range(xgb_num):\n","    model = joblib.load( filepath + models[i+cnn_num+xgb_num][0] )\n","    xgb_model_list_load.append(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FSA4RFm336Tt"},"source":["# Linear Regression"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nOVP1WXe36Tv","colab":{}},"source":["def to_data(input, cnn_model_list, inter_model_list, xgb_model_list):\n","\n","    pred_data = []\n","    a = len(cnn_model_list)\n","    b = len(xgb_model_list)\n","    for i in range(a):\n","           pred_data.append( cnn_model_list[i].predict(input) )\n","    for i in range(b):\n","        cnn_output = inter_model_list[i].predict( input )\n","        cnn_output = DataFrame(cnn_output)\n","        pred_data.append( xgb_model_list[i].predict_proba( cnn_output ) )\n","\n","    data = pred_data[0]\n","    for i in range(1,a+b):\n","        data = np.concatenate((data, pred_data[i]),axis=1)\n","    \n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zkRQzskA36T2","colab":{}},"source":["def linear_reg(aug_size, cnn_model_list, inter_model_list, xgb_model_list):\n","    \n","    train_aug = augmentation(train_copy, aug_size)\n","    input = train_aug.iloc[:,2:].values.copy().reshape(-1,28,28,1)\n","    data = to_data(input, cnn_model_list, inter_model_list, xgb_model_list)\n","    data_val = train_aug['digit']\n","    data_val = to_categorical(data_val, 10)\n","    x_train, x_val, y_train, y_val = train_test_split(data, data_val, test_size = 0.1)\n","    linear_model = LinearRegression()\n","    linear_model.fit(x_train, y_train)\n","    print('Coefficients: {}', format(linear_model.coef_[0][:10]))\n","    print(\"RSS: {}\".format( np.mean((linear_model.predict(x_val) - y_val) ** 2) ))\n","\n","    return linear_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vLHik4Iv36T9","colab":{}},"source":["train_aug_check = augmentation(train_copy,3)\n","x_check = train_aug_check.iloc[:,2:].values\n","x_check = x_check.reshape(-1,28,28,1)\n","y_check = train_aug_check['digit']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hFl9_VVB36UD","colab":{}},"source":["data_check = to_data(x_check, cnn_model_list_load, inter_model_list_load[], xgb_model_list_load[])\n","\n","linear_model = linear_reg(0, cnn_model_list_load, inter_model_list_load, xgb_model_list_load)\n","pred = linear_model.predict( data_check )\n","pred[:10]\n","pred_check = []\n","for i in range(len(pred)):\n","    pred_check.append( np.argmax(pred[i]) )\n","\n","s = [np.array(pred_check)==np.array(y_check)]\n","t = np.where(s[0]==True)\n","acc = len(t[0])/len(s[0])\n","acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pvkb1WOQ4yjX","colab_type":"code","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = to_data(x_test, cnn_model_list_load, inter_model_list_load, xgb_model_list_load)\n","\n","pred = linear_model.predict( data_test )\n","pred_test = []\n","for i in range(len(pred)):\n","    pred_test.append( np.argmax(pred[i]) )\n","pred_test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Drn6F7oy4YLh","colab_type":"code","colab":{}},"source":["# test cell\n","u = 1\n","v = 1\n","\n","data_check = to_data(x_check, cnn_model_list_load[:u], inter_model_list_load[2:2+v], xgb_model_list_load[2:2+v])\n","linear_model = linear_reg(3, cnn_model_list_load[:u], inter_model_list_load[2:2+v], xgb_model_list_load[2:2+v])\n","pred = linear_model.predict( data_check )\n","pred_check = []\n","for i in range(len(pred)):\n","    pred_check.append( np.argmax(pred[i]) )\n","\n","s = [np.array(pred_check)==np.array(y_check)]\n","t = np.where(s[0]==True)\n","acc = len(t[0])/len(s[0])\n","print('Predicted score : {}'.format(acc))\n","\n","x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = to_data(x_test, cnn_model_list_load[:u], inter_model_list_load[2:2+v], xgb_model_list_load[2:2+v])\n","\n","pred = linear_model.predict( data_test )\n","pred_test = []\n","for i in range(len(pred)):\n","    pred_test.append( np.argmax(pred[i]) )\n","pred_test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QrFql8jz36UN","colab":{}},"source":["data = {'id':test_copy['id'], 'digit':pred_test}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_1+1_bn_linearreg_2'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             'submission_89',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uy6vrgFN6zyf","colab_type":"code","colab":{}},"source":["pred_acc('submission_ensembles_6+2_bn_linearreg_2',file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p5lLhP79XMhe"},"source":["# ElasticNet Regression"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DuWXr39zXMhf","colab":{}},"source":["def to_data(input, cnn_model_list, inter_model_list, xgb_model_list):\n","\n","    pred_data = []\n","    a = len(cnn_model_list)\n","    b = len(xgb_model_list)\n","    for i in range(a):\n","           pred_data.append( cnn_model_list[i].predict(input) )\n","    for i in range(b):\n","        cnn_output = inter_model_list[i].predict( input )\n","        cnn_output = DataFrame(cnn_output)\n","        pred_data.append( xgb_model_list[i].predict_proba( cnn_output ) )\n","\n","    data = pred_data[0]\n","    for i in range(1,a+b):\n","        data = np.concatenate((data, pred_data[i]),axis=1)\n","    \n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jefPRRm3XMhi","colab":{}},"source":["def linear_reg(aug_size, cnn_model_list, inter_model_list, xgb_model_list):\n","    \n","    train_aug = augmentation(train_copy, aug_size)\n","    input = train_aug.iloc[:,2:].values.copy().reshape(-1,28,28,1)\n","    data = to_data(input, cnn_model_list, inter_model_list, xgb_model_list)\n","    data_val = train_aug['digit']\n","    data_val = to_categorical(data_val, 10)\n","    x_train, x_val, y_train, y_val = train_test_split(data, data_val, test_size = 0.1)\n","    linear_model = MultiTaskElasticNet(alpha=0.01)\n","    linear_model.fit(x_train, y_train)\n","    print('Coefficients: {}', format(linear_model.coef_[0][:10]))\n","    print(\"RSS: {}\".format( np.mean((linear_model.predict(x_val) - y_val) ** 2) ))\n","\n","    return linear_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H9LneTPoXMhk","colab":{}},"source":["train_aug_check = augmentation(train_copy,3)\n","x_check = train_aug_check.iloc[:,2:].values\n","x_check = x_check.reshape(-1,28,28,1)\n","y_check = train_aug_check['digit']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H2ovrLLiYVIz","colab_type":"code","colab":{}},"source":["data_check = to_data(x_check, cnn_model_list_load, inter_model_list_load, xgb_model_list_load)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0TVmbpsNXMhn","colab":{}},"source":["linear_model = linear_reg(3, cnn_model_list_load, inter_model_list_load, xgb_model_list_load)\n","pred = linear_model.predict( data_check )\n","pred[:10]\n","pred_check = []\n","for i in range(len(pred)):\n","    pred_check.append( np.argmax(pred[i]) )\n","\n","s = [np.array(pred_check)==np.array(y_check)]\n","t = np.where(s[0]==True)\n","acc = len(t[0])/len(s[0])\n","acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6g-k1EXgXMhp","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = to_data(x_test, cnn_model_list_load, inter_model_list_load, xgb_model_list_load)\n","\n","pred = linear_model.predict( data_test )\n","pred_test = []\n","for i in range(len(pred)):\n","    pred_test.append( np.argmax(pred[i]) )\n","pred_test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6IWv7taaXREe","colab_type":"code","colab":{}},"source":["data = {'id':test_copy['id'], 'digit':pred_test}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_10+5_bn_elasticreg_3_001'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             'submission_89',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1MH9F50gHjX","colab_type":"code","colab":{}},"source":["compare('submission_ensembles_10+5_bn_elasticreg_3_001','submission_ensembles_10+5_bn_elasticreg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wMQjuvMlHYSp","colab_type":"text"},"source":["# Ensembles : 10+5"]},{"cell_type":"code","metadata":{"id":"CQwqKDqhHaHK","colab_type":"code","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvges1MHIK-i","colab_type":"code","colab":{}},"source":["pred = ensemble(x_test, cnn_model_list_load, inter_model_list_load, xgb_model_list_load, 1.0, 3.0)\n","data = {'id':test_copy['id'], 'digit':pred}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_10+5_bn_30'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","print(pred[:10])\n","print()\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             'submission_89',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NYuwN37eMMOk","colab_type":"code","colab":{}},"source":["compare('submission_ensembles_10+5_bn_30','submission_ensembles_10+5_bn_25')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6_mzYDNIpxf","colab_type":"code","colab":{}},"source":["train_aug = augmentation(train_copy, 3)\n","x_check = train_aug.iloc[:,2:]\n","y_check = train_aug['digit']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8d1SPvTSJhMX","colab_type":"code","colab":{}},"source":["for i in range(10):    \n","    pred_check =  ensemble(x_check, cnn_model_list_load, inter_model_list_load, xgb_model_list_load, 1.0, 4+0.2*i)\n","    s = [np.array(pred_check)==np.array(y_check)]\n","    t = np.where(s[0]==True)\n","    acc = len(t[0])/len(s[0])\n","    print('{} Predicted score : {}'.format(2+0.2*i,acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZNpejLZvJ66T","colab_type":"code","colab":{}},"source":["pred_acc('submission_91_ensembles_6+2_bn_08',file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQw8SzHDvagk","colab_type":"text"},"source":["# Load models for ensemble : 5 models\n","\n"]},{"cell_type":"code","metadata":{"id":"zZcukIQXwedy","colab_type":"code","colab":{}},"source":["filepath = './drive/My Drive/DACON/saved_model/model_storage/'\n","\n","cnn = load_model( filepath + 'cnn_0_0.8891.hdf5' )\n","cnn_bn = load_model( filepath + 'cnn_bn_0.hdf5' )\n","depthwise = load_model( filepath + 'depthwise_0_0.8731.hdf5' )\n","pretrained = load_model( filepath + 'pretrained_by_letter_cnn_model_0_0.9038.hdf5' )\n","inter_model = load_model( filepath + 'inter_model.hdf5' )\n","xgb = joblib.load( filepath + 'xgb.dat' )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"48iI_SWTxjIB","colab_type":"code","colab":{}},"source":["model_list = []\n","model_list.append(cnn)\n","model_list.append(cnn_bn)\n","model_list.append(depthwise)\n","model_list.append(pretrained)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aT67cLsNyA3v","colab_type":"code","colab":{}},"source":["# simple ensemble\n","x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","\n","L = x_test.shape[0]\n","label_list = np.zeros((L,10))\n","for i in range(len(model_list)):\n","    label = model_list[i].predict( x_test )\n","    label_list += label\n","\n","cnn_output = inter_model.predict( x_test )\n","cnn_output = DataFrame(cnn_output)\n","label_xgb = xgb.predict_proba(cnn_output)\n","label_list += label_xgb*5.0\n","\n","pred = []\n","for j in range(len(label_list)):\n","    pred.append( np.argmax(label_list[j]) )\n","\n","pred[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16scCp50y0VD","colab_type":"code","colab":{}},"source":["data = {'id':test_copy['id'], 'digit':pred}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_w5'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             # 'submission_89',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPfFY3jSzEvF","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKy2kYXC2N6R","colab_type":"code","colab":{}},"source":["def data_for_reg(input):\n","    pred_data = []\n","    for i in range(len(model_list)):\n","        pred_data.append( model_list[i].predict( input ) )\n","    cnn_output = inter_model.predict( input )\n","    cnn_output = DataFrame(cnn_output)\n","    pred_data.append( xgb.predict_proba(cnn_output) )\n","\n","    data = pred_data[0]\n","    for i in range(1,5):\n","        data = np.concatenate((data,pred_data[i]),axis=1)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7dvCYgPe2jqg","colab_type":"code","colab":{}},"source":["\"\"\"# trian linear regression\n","train_aug = augmentation(train_copy,3)\n","x = train_aug.iloc[:,2:].values.reshape(-1,28,28,1)\n","data = data_for_reg( x )\n","data_val = train_aug['digit']\n","data_val = to_categorical(data_val, 10)\n","\n","x_train, x_val, y_train, y_val = train_test_split(data, data_val, test_size = 0.1)\n","linear_model = LinearRegression()\n","linear_model.fit(x_train, y_train)\n","print('Coefficients: {}', format(linear_model.coef_[0][:10]))\n","print(\"RSS: {}\".format( np.mean((linear_model.predict(x_val) - y_val) ** 2) ))\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ie024bl9-zcu","colab_type":"code","colab":{}},"source":["t_size = test_copy.shape[0]\n","x = np.divide(test_copy.iloc[:t_size,2:].values,255)\n","x = x.reshape(-1,28,28,1)\n","data = data_for_reg( x )\n","data_val = test_pred_copy['digit'][:t_size]\n","data_val = to_categorical(data_val, 10)\n","\n","x_train, x_val, y_train, y_val = train_test_split(data, data_val, test_size = 0.1)\n","linear_model = LinearRegression()\n","linear_model.fit(x_train, y_train)\n","print('Coefficients: {}', format(linear_model.coef_[0][:10]))\n","print(\"RSS: {}\".format( np.mean((linear_model.predict(x_val) - y_val) ** 2) ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgLaJfo22220","colab_type":"code","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = data_for_reg(x_test)\n","\n","pred = linear_model.predict( data_test )\n","pred_test = []\n","for i in range(len(pred)):\n","    pred_test.append( np.argmax(pred[i]) )\n","pred_test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNHCPQ8r3CKR","colab_type":"code","colab":{}},"source":["data = {'id':test_copy['id'], 'digit':pred_test}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_linear_using_test_all'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             # 'submission_89',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_90_ensembles_linear_using_test_1000',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWf3ICViqZ3-","colab_type":"code","colab":{}},"source":["def data_for_reg_1(input):\n","    pred_data = []\n","    for i in range(len(model_list)):\n","        label_proba = model_list[i].predict( input )\n","        label_list = []\n","        for j in range(label_proba.shape[0]):\n","            label = 0\n","            for k in range(10):\n","                label += label_proba[j][k]*k\n","            label_list.append( label )\n","        pred_data.append( np.array(label_list).reshape(-1,1) )\n","\n","    cnn_output = inter_model.predict( input )\n","    cnn_output = DataFrame(cnn_output)\n","    label_proba = xgb.predict_proba(cnn_output)\n","    label_list = []\n","    label = 0\n","    for j in range(label_proba.shape[0]):\n","        for k in range(10):\n","                label += label_proba[i][k]*k\n","        label_list.append( label )\n","    pred_data.append( np.array(label_list).reshape(-1,1) )\n","\n","    data = pred_data[0]\n","    for i in range(1,5):\n","        data = np.concatenate((data,pred_data[i]),axis=1)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMAXBsV7q2Aw","colab_type":"code","colab":{}},"source":["train_aug = augmentation(train_copy,3)\n","x = train_aug.iloc[:,2:].values.reshape(-1,28,28,1)\n","a = model_list[0].predict( x )\n","a.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M54M48OerBjP","colab_type":"code","colab":{}},"source":["# trian linear regression\n","train_aug = augmentation(train_copy,3)\n","x = train_aug.iloc[:,2:].values.reshape(-1,28,28,1)\n","data = data_for_reg_1( x )\n","data = np.divide(data,10)\n","data_val = train_aug['digit']\n","data_val = np.divide(data_val,10)\n","\n","x_train, x_val, y_train, y_val = train_test_split(data, data_val, test_size = 0.1)\n","linear_model = LinearRegression()\n","linear_model.fit(x_train, y_train)\n","print('Coefficients: {}', format(linear_model.coef_[0]))\n","print(\"RSS: {}\".format( np.mean((linear_model.predict(x_val) - y_val) ** 2) ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Gg2dvXbwkn9","colab_type":"code","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = data_for_reg_1(x_test)\n","\n","pred = linear_model.predict( data_test )\n","pred[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRDvrY-Ayn6n","colab_type":"code","colab":{}},"source":["def data_for_reg_clip(input, clip):\n","    pred_data = []\n","    for i in range(len(model_list)):\n","        label = model_list[i].predict( input )\n","        label[label<clip]=0\n","        pred_data.append( label )\n","    cnn_output = inter_model.predict( input )\n","    cnn_output = DataFrame(cnn_output)\n","    label = xgb.predict_proba(cnn_output)\n","    label[label<clip]=0\n","    pred_data.append( label )\n","\n","    data = pred_data[0]\n","    for i in range(1,5):\n","        data = np.concatenate((data,pred_data[i]),axis=1)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1-09d_KDzfE","colab_type":"code","colab":{}},"source":["t_size = 1000\n","x = np.divide(test_copy.iloc[:t_size,2:].values,255)\n","x = x.reshape(-1,28,28,1)\n","data = data_for_reg_clip( x, 0.001 )\n","data_val = test_pred_copy['digit'][:t_size]\n","data_val = to_categorical(data_val, 10)\n","\n","x_train, x_val, y_train, y_val = train_test_split(data, data_val, test_size = 0.1)\n","linear_model = LinearRegression()\n","linear_model.fit(x_train, y_train)\n","print('Coefficients: {}', format(linear_model.coef_[0][:10]))\n","print(\"RSS: {}\".format( np.mean((linear_model.predict(x_val) - y_val) ** 2) ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3YL7eMJlEM8l","colab_type":"code","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = data_for_reg(x_test)\n","\n","pred = linear_model.predict( data_test )\n","pred_test = []\n","for i in range(len(pred)):\n","    pred_test.append( np.argmax(pred[i]) )\n","pred_test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GF-JTTsqERL1","colab_type":"code","colab":{}},"source":["data = {'id':test_copy['id'], 'digit':pred_test}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_linear_using_test_clip'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             # 'submission_89',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_90_ensembles_linear_using_test_1000',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pjwsRk0lEWOf","colab_type":"code","colab":{}},"source":["train_aug = augmentation(train_copy,0)\n","x = train_aug.iloc[:,2:].values.reshape(-1,28,28,1)\n","data = data_for_reg_clip( x,0.001 )\n","data_val = train_aug['digit']\n","data_val = to_categorical(data_val, 10)\n","\n","x_train, x_val, y_train, y_val = train_test_split(data, data_val, test_size = 0.1)\n","linear_model = LinearRegression()\n","linear_model.fit(x_train, y_train)\n","print('Coefficients: {}', format(linear_model.coef_[0][:10]))\n","print(\"RSS: {}\".format( np.mean((linear_model.predict(x_val) - y_val) ** 2) ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAgWgcSlEmTE","colab_type":"code","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = data_for_reg(x_test)\n","\n","pred = linear_model.predict( data_test )\n","pred_test = []\n","for i in range(len(pred)):\n","    pred_test.append( np.argmax(pred[i]) )\n","pred_test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ir2ICg1nGY5x","colab_type":"code","colab":{}},"source":["data = {'id':test_copy['id'], 'digit':pred_test}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_linear_clip_aug0'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             # 'submission_89',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_90_ensembles_linear_using_test_1000',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJERrX-3PUv1","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cx061krOPVmD","colab_type":"text"},"source":["# Stacking model"]},{"cell_type":"code","metadata":{"id":"dK6ieYj2GdzX","colab_type":"code","colab":{}},"source":["def stack_model( input , N):\n","    stack = Sequential()\n","    stack.add(Input(shape=(50,)))\n","    stack.add(Dense(N, activation = \"relu\"))\n","    stack.add(BatchNormalization())\n","    # stack.add(Dropout(0.5))\n","    \n","    stack.add(Dense(10, activation = \"softmax\"))\n","\n","    # fit model\n","    batch_size = 100\n","    epochs = 10\n","\n","    data = data_for_reg( input.iloc[:,2:].values.reshape(-1,28,28,1) )\n","    data_val = input['digit']\n","    data_val = to_categorical(data_val, 10)\n","\n","    x_train, x_val, y_train, y_val = train_test_split(data,data_val,test_size=0.1,random_state=15)\n","\n","    optimizer = RMSprop(lr=0.01, rho=0.9, epsilon=1e-08, decay=0.0)\n","    stack.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    hist = stack.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n","                validation_data = (x_val,y_val),\n","                steps_per_epoch=x_train.shape[0]// batch_size\n","                )\n","    stack.summary()\n","\n","    return stack\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGhMiEzmKroF","colab_type":"code","colab":{}},"source":["train_aug = augmentation(train_copy,3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x4Pl3xIbJPAP","colab_type":"code","colab":{}},"source":["stack = stack_model( train_aug, 256 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kpaD5zRKVSz","colab_type":"code","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = data_for_reg(x_test)\n","\n","pred = stack.predict( data_test )\n","pred_test = []\n","for i in range(len(pred)):\n","    pred_test.append( np.argmax(pred[i]) )\n","pred_test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hymyy59-Ldm4","colab_type":"code","colab":{}},"source":["data = {'id':test_copy['id'], 'digit':pred_test}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_stack_bn_do_2_256'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             # 'submission_89',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_90_ensembles_linear_using_test_1000',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVWw_ua0PiIS","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADE-RizIPjyX","colab_type":"text"},"source":["# Load more models"]},{"cell_type":"code","metadata":{"id":"CyZLko2SLiNe","colab_type":"code","colab":{}},"source":["filepath = './drive/My Drive/DACON/saved_model/model_storage/'\n","\n","cnn = load_model( filepath + 'cnn_0_0.8891.hdf5' )\n","cnn_1 = load_model( filepath + 'cnn_1_0.8975.hdf5' )\n","# cnn_2 = load_model( filepath + 'cnn_2_0.9100.hdf5' )\n","cnn_bn = load_model( filepath + 'cnn_bn_0.hdf5' )\n","cnn_bn_1 = load_model( filepath + 'cnn_bn_1.hdf5' )\n","depthwise = load_model( filepath + 'depthwise_0_0.8731.hdf5' )\n","# depthwise_2 = load_model( filepath + 'depthwise_2_0.8577.hdf5' )\n","# depthwise_1 = load_model( filepath + 'depthwise_1_0.8529.hdf5' )\n","pretrained = load_model( filepath + 'pretrained_by_letter_cnn_model_0_0.9038.hdf5' )\n","inter_model = load_model( filepath + 'inter_model.hdf5' )\n","xgb = joblib.load( filepath + 'xgb.dat' )\n","inter_model_1 = load_model( filepath + 'inter_model_1.hdf5' )\n","xgb_1 = joblib.load( filepath + 'xgb_1.dat' )\n","\n","vgg_0 = load_model(filepath + 'vgg_0_0.9163.hdf5' )\n","vgg_1 = load_model(filepath + 'vgg_1_0.8906.hdf5' )\n","# vgg_2 = load_model(filepath + 'vgg_2_0.9053.hdf5' )\n","\n","res_0 = load_model(filepath + 'ResNet_0_0.9066.hdf5' )\n","res_1 = load_model(filepath + 'ResNet_1_0.9261.hdf5' )\n","res_2 = load_model(filepath + 'ResNet_2_0.8996.hdf5' )\n","\n","model_list = []\n","model_list.append(cnn)\n","model_list.append(cnn_1)\n","# model_list.append(cnn_2)\n","model_list.append(cnn_bn)\n","model_list.append(cnn_bn_1)\n","model_list.append(depthwise)\n","# model_list.append(depthwise_1)\n","# model_list.append(depthwise_2)\n","model_list.append(pretrained)\n","model_list.append(vgg_0)\n","model_list.append(vgg_1)\n","# model_list.append(vgg_2)\n","model_list.append(res_0)\n","model_list.append(res_1)\n","model_list.append(res_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xynOey_1RFKA","colab_type":"code","colab":{}},"source":["inter_model_list = []\n","inter_model_list.append( inter_model )\n","inter_model_list.append( inter_model_1 )\n","\n","xgb_list = []\n","xgb_list.append( xgb )\n","xgb_list.append( xgb_1 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gegE6CjhRJAq","colab_type":"code","colab":{}},"source":["def data_for_reg_more(input):\n","    pred_data = []\n","    for i in range(len(model_list)):\n","        pred_data.append( model_list[i].predict( input ) )\n","        print(i)\n","    for j in range( len(inter_model_list) ):\n","        cnn_output = inter_model_list[j].predict( input )\n","        cnn_output = DataFrame(cnn_output)\n","        pred_data.append( xgb_list[j].predict_proba(cnn_output) )\n","        print(j)\n","\n","    data = pred_data[0]\n","    for i in range(1,13):\n","        data = np.concatenate((data,pred_data[i]),axis=1)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lbCMpJiaShS9","colab_type":"code","colab":{}},"source":["def stack_model_more( input_imgs , input_digit, model_num, N):\n","    stack = Sequential()\n","    stack.add(Input(shape=(10*model_num,)))\n","    stack.add(Dense(N, activation = \"relu\"))\n","    stack.add(BatchNormalization())\n","    # stack.add(Dropout(0.5))\n","\n","    stack.add(Dense(2*N, activation = \"relu\"))\n","    stack.add(BatchNormalization())\n","    \n","    stack.add(Dense(10, activation = \"softmax\"))\n","\n","    # fit model\n","    batch_size = 500\n","    epochs = 100\n","\n","    data = data_for_reg_more( input_imgs.values.reshape(-1,28,28,1) )\n","    data_val = input_digit\n","    data_val = to_categorical(data_val, 10)\n","\n","    x_train, x_val, y_train, y_val = train_test_split(data,data_val,test_size=0.1,random_state=15)\n","\n","    optimizer = RMSprop(lr=0.01, rho=0.9, epsilon=1e-08, decay=0.0)\n","    stack.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    file_name = 'model_stack'\n","    model_path = set_filepath(file_name) + 'stack_{val_loss:.4f}.hdf5'\n","\n","    # callbacks\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n","    mcp_save = ModelCheckpoint(filepath = model_path, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n","\n","    hist = stack.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n","                validation_data = (x_val,y_val),\n","                steps_per_epoch=x_train.shape[0]// batch_size\n","                , callbacks = [early_stopping, mcp_save]\n","                )\n","    stack.summary()\n","\n","    return stack\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VX337D2bR7hl","colab_type":"code","colab":{}},"source":["stack = stack_model_more( train_aug.iloc[:,2:], train_aug['digit'], 13, 64 )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzQUJWxeSVxH","colab_type":"code","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = data_for_reg_more(x_test)\n","\n","pred = stack.predict( data_test )\n","pred_test = []\n","for i in range(len(pred)):\n","    pred_test.append( np.argmax(pred[i]) )\n","pred_test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGjHCSCBS296","colab_type":"code","colab":{}},"source":["data = {'id':test_copy['id'], 'digit':pred_test}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_stack_more+_using_test'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             'submission_89_ensembles_stack_more',\n","             # 'submission_89',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_90_ensembles_linear_using_test_1000',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7K8L13Bihhmy","colab_type":"text"},"source":["# Using predicted data"]},{"cell_type":"code","metadata":{"id":"DjmtQS4WeBbt","colab_type":"code","colab":{}},"source":["pred1 = pd.read_csv('./drive/My Drive/DACON/submission/submission_90_ensembles_stack_more+_using_test.csv').copy()\n","pred2 = pd.read_csv('./drive/My Drive/DACON/submission/submission_91_ensembles_6+2_bn_08.csv').copy()\n","pred3 = pd.read_csv('./drive/My Drive/DACON/submission/submission_91_ensembles_3+1_w1.csv').copy()\n","\n","overlap_id1 = np.where([pred1['digit']==pred2['digit']][0]==True)[0]\n","overlap_id2 = np.where([pred1['digit']==pred3['digit']][0]==True)[0]  \n","overlap_id = np.array([id for id in overlap_id1 if id in overlap_id2])\n","print(len(overlap_id))\n","\n","aug_data = pd.concat([pred1.iloc[overlap_id,0:2], test_copy.iloc[overlap_id,1:]], axis=1)\n","\n","aug = augmentation(aug_data, 2)\n","input_test = aug.iloc[:,2:]\n","input_digit = aug['digit']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmrjgZdQ1OnD","colab_type":"code","colab":{}},"source":["data_reg = data_for_reg_more( input_test.values.reshape(-1,28,28,1) )\n","data_val = input_digit\n","data_val = to_categorical(data_val, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0akOiI2I2gvY","colab_type":"code","colab":{}},"source":["train_aug = augmentation(train_copy,2)\n","data_reg1 = data_for_reg_more( train_aug.iloc[:,2:].values.reshape(-1,28,28,1) )\n","data_val1 = train_aug['digit']\n","data_val1 = to_categorical(data_val1,10)\n","x = np.concatenate([data_reg, data_reg1], axis=0)\n","y = np.concatenate([data_val, data_val1], axis=0)\n","x_train, x_val, y_train, y_val = train_test_split(data_reg1, data_val1,test_size=0.1,random_state=24)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XggqOLwn3NAm","colab_type":"code","colab":{}},"source":["x_train, x_val, y_train, y_val = train_test_split(data_reg , data_val,test_size=0.1,random_state=24)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tonyE565stA6","colab_type":"code","colab":{}},"source":["\"\"\"x_train = data\n","y_train = data_val\n","\n","train_aug = augmentation(train_copy,3)\n","x_val = data_for_reg_more( train_aug.iloc[:,2:].values.reshape(-1,28,28,1) )\n","y_val = train_aug['digit']\n","y_val = to_categorical(y_val, 10)\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fo6vh2QQ1Ebt","colab_type":"code","colab":{}},"source":["model_num = 13\n","\n","N = 256\n","stack = Sequential()\n","stack.add(Input(shape=(10*model_num,)))\n","stack.add(Dense(N, activation = \"relu\"))\n","# stack.add(BatchNormalization())\n","stack.add(Dense(2*N, activation = \"relu\"))\n","# stack.add(BatchNormalization())\n","stack.add(Dense(4*N, activation = \"relu\"))\n","# stack.add(BatchNormalization())\n","stack.add(Dense(8*N, activation = \"relu\"))\n","# stack.add(BatchNormalization())\n","# stack.add(Dense(16*N, activation = \"relu\"))\n","# stack.add(BatchNormalization())\n","stack.add(Dense(10, activation = \"softmax\"))\n","\n","# fit model\n","batch_size = 200\n","epochs = 10\n","# optimizer = RMSprop(lr=0.01, rho=0.9, epsilon=1e-08, decay=0.0)\n","optimizer = Adam()\n","stack.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","file_name = 'model_stack'\n","model_path = set_filepath(file_name) + 'stack_{val_accuracy:.4f}.hdf5'\n","\n","# callbacks\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n","mcp_save = ModelCheckpoint(filepath = model_path, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n","\n","hist = stack.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n","            validation_data = (x_val,y_val),\n","            steps_per_epoch=x_train.shape[0]// batch_size\n","            # , callbacks = [early_stopping, mcp_save]\n","            )\n","# stack = load_best('model_stack')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ud7YznPCfczU","colab_type":"code","colab":{}},"source":["x_test = np.divide(test_copy.iloc[:,2:].values,255)\n","x_test = x_test.reshape(-1,28,28,1)\n","data_test = data_for_reg_more(x_test)\n","\n","pred = stack.predict( data_test )\n","pred_test = []\n","for i in range(len(pred)):\n","    pred_test.append( np.argmax(pred[i]) )\n","pred_test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LB_LVRqOfn33","colab_type":"code","colab":{}},"source":["data = {'id':test_copy['id'], 'digit':pred_test}\n","submission = DataFrame(data)\n","file_name = 'submission_ensembles_stack_more+res_13_256'\n","submission.to_csv('./drive/My Drive/DACON/submission/'+file_name+'.csv', index=False)\n","\n","file_list = [ 'submission_84',\n","             'submission_85',\n","             'submission_86_xgb_ensemble',\n","             'submission_87_ensembles',\n","             'submission_87_ensembles_10+5_bn_linearreg',\n","             'submission_87_ensembles_stack_more+res_13_256',\n","             'submission_88_ensemble_2_2_4_try3',\n","             'submission_88_ensembles_10+1_bn_linearreg',\n","             'submission_88_ensembles_6+2_bn_linearreg_2',\n","             'submission_89_ensemble_2_2',\n","             'submission_89_ensembles_stack_more',\n","             'submission_89_ensembles_stack_more_using_test',\n","             'submission_89_ensembles_stack_more++_using_test_overlap_wobn_512',\n","             'submission_90_ensembles_6+2_bn_08_retry',\n","             'submission_90_pretrain_using_test_layer_4_3ensemble',\n","             'submission_90_ensembles_6+2_bn_linearreg',\n","             'submission_90_ensembles_linear_using_test_1000',\n","             'submission_90_ensembles_stack_more+_using_test',\n","             'submission_91_ensembles_3+1_w1',\n","             'submission_91_ensembles_6+2_bn_08',\n","             'submission_91_ensembles_stack_more++_using_test_overlap_909191_2048_aug',\n","             'submission_91_ensembles_stack_more++_using_test_overlap_909191_64_aug'\n","             ]\n","pred_acc(file_name,file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMyDbzyuj4iq","colab_type":"code","colab":{}},"source":["pred_acc('submission_89_ensembles_stack_more++_using_test_overlap_wobn_512',file_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKFkeopuY_re","colab_type":"code","colab":{}},"source":["compare('submission_ensembles_stack_more+res_using_test_overlap_wobn_512', file_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PAOysRA8SIVb","colab_type":"text"},"source":["# Train VGG"]},{"cell_type":"code","metadata":{"id":"7rLX7NulE99v","colab_type":"code","colab":{}},"source":["def get_vgg_model( init_dim ):\n","    model = Sequential()\n","    model.add(Conv2D(init_dim, (3, 3), activation='relu', padding=\"same\", input_shape=(28, 28, 1)))\n","    model.add(Conv2D(init_dim, (3, 3), activation='relu', padding=\"same\"))\n","\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(init_dim*2, (3, 3), activation='relu', padding=\"same\"))\n","    model.add(Conv2D(init_dim*2, (3, 3), activation='relu', padding=\"same\"))\n","\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(init_dim*4, (3, 3), activation='relu', padding=\"same\"))\n","    model.add(Conv2D(init_dim*4, (3, 3), activation='relu', padding=\"same\"))\n","    model.add(Conv2D(init_dim*4, (3, 3), activation='relu', padding=\"same\"))\n","\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dense(1024, activation='relu'))\n","    model.add(Dense(10, activation='softmax'))\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U48zHUqOFPV5","colab_type":"code","colab":{}},"source":["batch_size = 100\n","epochs = 100\n","\n","model = get_vgg_model(64)\n","file_name = 'model_vgg'\n","model_path = set_filepath(file_name) + 'vgg' + '_{val_accuracy:.4f}.hdf5'\n","\n","# callbacks\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='max')\n","mcp_save = ModelCheckpoint(filepath = model_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n","reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n","# compile\n","optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n","model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","# fit model\n","x_train, x_val, y_train, y_val = train_test_gen(train_copy, 2)\n","\n","hist = model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n","            validation_data = (x_val,y_val),\n","            steps_per_epoch=x_train.shape[0]// batch_size, \n","            callbacks=[early_stopping,mcp_save,reduce_lr_loss])\n","model = load_best(file_name)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUKYbb2VFnY_","colab_type":"code","colab":{}},"source":["a = test_copy.iloc[5,2:].values.astype(np.int).reshape(28,28)\n","plt.imshow(a,cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4u0TZjW1KR3D","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vx9XkP1cAcEs","colab_type":"text"},"source":["# ResNet"]},{"cell_type":"code","metadata":{"id":"Mq9Dy7GmAd6f","colab_type":"code","colab":{}},"source":["def get_ResNet(learning_rate):\n","    \n","    # Remove the previous model.\n","    model = None\n","    \n","    # Input layer\n","    img_input = Input(shape = (28,28,1))\n","    \n","    # CNN\n","    # Identity mapping shortcut을 위한 conv_1 layer\n","    conv_1 = Conv2D(128, kernel_size = 3, padding = 'same', activation = 'relu')(img_input) \n","    \n","    conv_2_1 = Conv2D(128, kernel_size = 3, padding = 'same', activation = 'relu')(conv_1)\n","    conv_2_1 = Conv2D(128, kernel_size = 3, padding = 'same')(conv_2_1)\n","    \n","    # ShortCut connection\n","    add_2_1 = add([conv_1, conv_2_1])\n","    out_2_1 = Activation('relu')(add_2_1)\n","    \n","    conv_2_2 = Conv2D(128, kernel_size = 3, padding = 'same', activation = 'relu')(out_2_1)\n","    conv_2_2 = Conv2D(128, kernel_size = 3, padding = 'same')(conv_2_2)\n","    \n","    # ShortCut connection\n","    add_2_2 = add([out_2_1, conv_2_2])\n","    out_2_2 = Activation('relu')(add_2_1)\n","    \n","    pool_2 = MaxPool2D((2,2), strides = 2)(out_2_2)\n","    \n","    conv_3_0 = Conv2D(256, kernel_size = 1, strides = 1)(pool_2)\n","    \n","    conv_3_1 = Conv2D(256, kernel_size = 3, padding = 'same', activation = 'relu')(conv_3_0)\n","    conv_3_1 = Conv2D(256, kernel_size = 3, padding = 'same')(conv_3_1)\n","    \n","    # ShortCut connection\n","    add_3_1 = add([conv_3_0, conv_3_1])\n","    out_3_1 = Activation('relu')(add_3_1)\n","    \n","    conv_3_2 = Conv2D(256, kernel_size = 3, padding = 'same', activation = 'relu')(out_3_1)\n","    conv_3_2 = Conv2D(256, kernel_size = 3, padding = 'same')(conv_3_2)\n","    \n","    # ShortCut connection\n","    add_3_2 = add([out_3_1, conv_3_2])\n","    out_3_2 = Activation('relu')(add_3_2)\n","    \n","    pool_3 = MaxPool2D((2,2), strides = 2)(out_3_2)\n","    \n","    conv_4_0 = Conv2D(256, kernel_size = 1, strides = 1)(pool_3)\n","    \n","    conv_4_1 = Conv2D(256, kernel_size = 3, padding = 'same', activation = 'relu')(conv_4_0)\n","    conv_4_1 = Conv2D(256, kernel_size = 3, padding = 'same')(conv_4_1)\n","    \n","    # ShortCut connection\n","    add_4_1 = add([conv_4_0, conv_4_1])\n","    out_4_1 = Activation('relu')(add_4_1)\n","    \n","    pool_4 = MaxPool2D((2,2), strides = 2)(out_4_1)\n","    \n","    # FC layers\n","    img_features = Flatten()(pool_4)\n","    img_features = Dense(512, activation = 'relu')(img_features)\n","    img_features = Dropout(rate = 0.5)(img_features)\n","    img_features = Dense(512, activation = 'relu')(img_features)\n","    img_features = Dropout(rate = 0.5)(img_features)\n","    \n","    # Output layer\n","    digit_pred = Dense(10, activation = 'softmax')(img_features)\n","    \n","    model = Model(inputs = img_input, outputs = digit_pred)\n","    \n","    model.compile(optimizer = Adam(lr = learning_rate),\n","                 loss = 'categorical_crossentropy',\n","                 metrics = ['accuracy'])\n","                    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iIVpVv3yA800","colab_type":"code","colab":{}},"source":["batch_size = 100\n","epochs = 100\n","\n","model = get_ResNet(0.0001)\n","file_name = 'model_ResNet'\n","model_path = set_filepath(file_name) + 'ResNet' + '_{val_accuracy:.4f}.hdf5'\n","\n","# callbacks\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n","mcp_save = ModelCheckpoint(filepath = model_path, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n","# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n","\"\"\"# compile\n","optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n","model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\"\"\"\n","# fit model\n","x_train, x_val, y_train, y_val = train_test_gen(train_copy, 3)\n","\n","hist = model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, \n","            validation_data = (x_val,y_val),\n","            steps_per_epoch=x_train.shape[0]// batch_size, \n","            callbacks=[early_stopping,mcp_save])\n","model = load_best(file_name)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jheycKBGB_nA","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}