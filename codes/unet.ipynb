{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"project_unet.ipynb","provenance":[{"file_id":"1uZZrMRg5GDFr3w8pDErFdY-1XBs1ZS_t","timestamp":1598207300314}],"private_outputs":true,"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"scrolled":true,"id":"VcrtophXhy-u","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import seaborn as sns\n","%matplotlib inline\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ReduceLROnPlateau\n","\n","sns.set(style='white', context='notebook', palette='deep')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UTAyem3Ehy-z","colab_type":"code","colab":{}},"source":["# pandas version\n","print('pandas',pd.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUWzmirvi7uZ","colab_type":"code","colab":{}},"source":["# define train set\n","from google.colab import drive\n","drive.mount('/content/drive')\n","train = pd.read_csv('./drive/My Drive/DACON/data/train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"GzH6dvcNhy_H","colab_type":"code","colab":{}},"source":["train_img = train.copy()\n","\n","# reshape and normalization\n","x_28 = np.divide(train_img.iloc[:,3:].values.copy(),255).reshape(-1,28,28,1)\n","x_train = np.zeros((x_28.shape[0],112,112,1))\n","\n","plt.figure(figsize = (10,10))\n","plt.subplot(1,2,1)\n","plt.imshow(x_28[0].reshape(28,28),cmap='gray')\n","\n","npad = ((42,42),(42,42))\n","for i in range(x_28.shape[0]):\n","    x_train[i] = np.pad(x_28[i].reshape(28,28),npad,'constant',constant_values=(0)).reshape(112,112,1)\n","\n","plt.subplot(1,2,2)\n","plt.imshow(x_train[0].reshape(112,112),cmap='gray')\n","\n","# label encoding\n","y_28 = train_img.iloc[:,3:].copy()\n","\n","train_copy = train.copy()\n","train_img_avg = np.zeros((10,26,784))\n","\n","for i in range(10):\n","    for j in range(26):\n","        a = train_copy[(train_copy['digit']==i) & (train_copy['letter']==chr(j+65))]\n","        b = a.iloc[0,3:]\n","        for k in range(1,len(a)-1):\n","            b += a.iloc[k,3:] \n","        b = np.divide(b,len(a))\n","        train_img_avg[i][j] = b\n","\n","for i in range(train_img.shape[0]):\n","    y_28.iloc[i,:] = train_img_avg[train_img.iloc[i,1]][ord(train_img.iloc[i,2])-65] \n","y_28 = np.divide(y_28.values,255).reshape(-1,28,28,1)\n","y_train = np.zeros((y_28.shape[0],112,112,1))\n","\n","plt.figure(figsize = (10,10))\n","plt.subplot(1,2,1)\n","plt.imshow(x_28[0].reshape(28,28),cmap='gray')\n","\n","npad = ((42,42),(42,42))\n","for i in range(y_28.shape[0]):\n","    y_train[i] = np.pad(y_28[i].reshape(28,28),npad,'constant',constant_values=(0)).reshape(112,112,1)\n","\n","plt.subplot(1,2,2)\n","plt.imshow(y_train[0].reshape(112,112),cmap='gray')\n","\n","# split training and validation set\n","x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoOkmf0fhy_M","colab_type":"code","colab":{}},"source":["# set the CNN model \n","# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n","\n","def unet(input_size = (112,112,1)):\n","    inputs = Input(input_size)\n","    N = 16\n","    conv1 = Conv2D(N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n","    conv1 = Conv2D(N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","    conv2 = Conv2D(2*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n","    conv2 = Conv2D(2*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","    conv3 = Conv2D(4*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","    conv3 = Conv2D(4*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","    conv4 = Conv2D(8*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","    conv4 = Conv2D(8*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n","    drop4 = Dropout(0.5)(conv4)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","    conv5 = Conv2D(16*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","    conv5 = Conv2D(16*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","    drop5 = Dropout(0.5)(conv5)\n","\n","    up6 = Conv2D(8*N, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n","    merge6 = concatenate([drop4,up6], axis = 3)\n","    conv6 = Conv2D(8*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","    conv6 = Conv2D(8*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","\n","    up7 = Conv2D(4*N, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","    merge7 = concatenate([conv3,up7], axis = 3)\n","    conv7 = Conv2D(4*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","    conv7 = Conv2D(4*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","\n","    up8 = Conv2D(2*N, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","    merge8 = concatenate([conv2,up8], axis = 3)\n","    conv8 = Conv2D(2*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","    conv8 = Conv2D(2*N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","\n","    up9 = Conv2D(N, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n","    merge9 = concatenate([conv1,up9], axis = 3)\n","    conv9 = Conv2D(N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","    conv9 = Conv2D(N, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n","\n","    model = Model(inputs = inputs, outputs = conv10)\n","\n","    model.compile(optimizer = Adam(lr = 1e-4), loss = \"binary_crossentropy\", metrics = ['accuracy'])\n","    \n","    # model.summary()\n","\n","    # if(pretrained_weights): model.load_weights(pretrained_weights)\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"fyjzmo7Vhy_O","colab_type":"code","colab":{}},"source":["# with data augmentation to prevent overfitting \n","\n","datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n","        zoom_range = 0.1, # Randomly zoom image \n","        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n","        horizontal_flip=False,  # randomly flip images\n","        vertical_flip=False)  # randomly flip images\n","\n","\n","datagen.fit(x_train)\n","\n","# set a learning rate annealer\n","learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n","                                            patience=3, \n","                                            verbose=1, \n","                                            factor=0.5, \n","                                            min_lr=0.00001)\n","\n","# fit the model\n","\n","epochs = 5\n","batch_size = 50\n","\n","model_u = unet()\n","history = model_u.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n","                              epochs = epochs, \n","                              validation_data = (x_val,y_val),\n","                              steps_per_epoch=x_train.shape[0]// batch_size\n","                              ,callbacks=[learning_rate_reduction]\n","                             )\n","\n","# plot the loss and accuracy curves for training and validation \n","fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n","ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1kxqDgxHJaT","colab_type":"code","colab":{}},"source":["z = np.divide(train.copy().iloc[:,3:].values,255).reshape(-1,28,28,1)\n","w = np.zeros((z.shape[0],112,112,1))\n","\n","npad = ((42,42),(42,42))\n","for i in range(z.shape[0]):\n","    w[i] = np.pad(z[i].reshape(28,28),npad,'constant',constant_values=(0)).reshape(112,112,1)\n","ip =  np.divide(w[1],255).reshape(1,112,112,1)\n","print(ip.shape)\n","plt.imshow(ip.reshape(112,112),cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJbgxhg7Jx4h","colab_type":"code","colab":{}},"source":["op = model_u.predict(ip)\n","plt.imshow(op.reshape(112,112),cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bBmL1Ytvi0L","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}